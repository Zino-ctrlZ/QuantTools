# ruff: noqa
"""
Multi-Ticker Performance Test - 10 Option Tickers
Tests caching performance across multiple tickers with 60 business days each
"""

import pandas as pd
import time
from pandas.tseries.offsets import BDay
from trade.helpers.helper import parse_option_tick
from DataManagers.DataManagers_cached import CachedOptionDataManager
from DataManagers import set_skip_mysql_query
from DataManagers.option_cache.helpers import load_option_data_cache

# Skip MySQL for performance
set_skip_mysql_query(True)

def get_expiration_date(opttick: str) -> pd.Timestamp:
    """Extract expiration date from option ticker."""
    parsed = parse_option_tick(opttick)
    return pd.Timestamp(parsed['exp'])

print("="*80)
print("MULTI-TICKER PERFORMANCE TEST - 10 OPTION TICKERS")
print("="*80)

# Test tickers - expanded list (40 tickers)
tickers = [
    'AMZN20240920C195',
    'AMZN20240920C185',
    'AMZN20250117C195',
    'AMZN20250117C200',
    'AMD20250117C165',
    'AAPL20241220C215',
    'AAPL20250620C250',
    'AAPL20250620C260',
    'TSLA20250321C265',
    'TSLA20250321C260',
    'AMD20250321C210',
    'AMD20250321C200',
    'META20250620C590',
    'META20250620C580',
    'SBUX20250321C95',
    'SBUX20250321C90',
    'TSLA20250620C250',
    'TSLA20250620C240',
    'META20250620C660',
    'META20250620C670',
    'AMD20250620C220',
    'AMD20250620C210',
    'TSLA20250815C320',
    'TSLA20250815C330',
    'AMZN20250620C220',
    'AMZN20250620C215',
    'SBUX20250718C105',
    'SBUX20250718C100',
    'TSLA20250919C490',
    'TSLA20250919C480',
    'BA20250919C250',
    'BA20250919C260',
    'BA20250919C200',
    'BA20250919C210',
    'AMD20260618C270',
    'AMD20260618C280',
    'AAPL20260821C290',
    'AAPL20260821C300',
    'BA20260618C275',
    'BA20260618C270'
]

print(f"\nTesting {len(tickers)} option tickers")
print(f"Each query: 60 business days (Exp - 60 BDay to Exp)")
print("\nTickers:")
for i, tick in enumerate(tickers, 1):
    print(f"  {i}. {tick}")

# Test configuration flags
ENABLE_PARTIAL_CACHE_TEST = True  # Set to False to skip partial cache tests
DROP_TODAY_FROM_CACHE = False  # Set to True to drop today's data when caching

print(f"\nTest Configuration:")
print(f"  ‚Ä¢ Partial cache test: {'ENABLED' if ENABLE_PARTIAL_CACHE_TEST else 'DISABLED'}")
print(f"  ‚Ä¢ Drop today from cache: {'YES' if DROP_TODAY_FROM_CACHE else 'NO'}")

# Clear all caches for fresh start
print("\nClearing caches...")
spot_cache, bs_vol, bs_greeks, binom_vol, binom_greeks = load_option_data_cache()
spot_cache.clear()
bs_vol.clear()
bs_greeks.clear()
binom_vol.clear()
binom_greeks.clear()
print("‚úì All caches cleared\n")

# Results storage
results = []

# =============================================================================
# FIRST PASS: Cache MISS - Query all tickers (uncached)
# =============================================================================
print("="*80)
print("PASS 1: UNCACHED QUERIES (Cache MISS)")
print("="*80)

total_time_uncached = 0
total_rows_uncached = 0

for i, opttick in enumerate(tickers, 1):
    print(f"\n[{i}/{len(tickers)}] {opttick}")
    print("-" * 60)
    
    # Parse ticker
    parsed = parse_option_tick(opttick)
    symbol = parsed['ticker']
    exp_date = parsed['exp_date']
    right = parsed['put_call']
    strike = parsed['strike']
    exp = pd.to_datetime(exp_date)
    
    # Smart date range: if exp > today, use today-60 to today; else use exp-60 to exp
    today = pd.Timestamp.now().normalize()
    if exp > today:
        start = today - BDay(60)
        end = today
        print(f"  ‚ö†Ô∏è Future expiration ({exp.date()}), using today-60 to today: {start.date()} to {end.date()}")
    else:
        start = exp - BDay(60)
        end = exp
        print(f"  ‚ÑπÔ∏è Past expiration ({exp.date()}), using exp-60 to exp: {start.date()} to {end.date()}")
    
    # Create manager
    manager = CachedOptionDataManager(
        symbol=symbol,
        exp=exp_date,
        right=right,
        strike=strike
    )
    
    # Test GREEKS (includes spot + vol cascade)
    start_time = time.time()
    result = manager.get_timeseries(start, end, interval='1d', type_='greeks', model='bs')
    elapsed = time.time() - start_time
    
    rows = len(result.post_processed_data)
    total_time_uncached += elapsed
    total_rows_uncached += rows
    
    print(f"  ‚úì Greeks: {rows} rows in {elapsed:.3f}s")
    
    # Store result
    results.append({
        'ticker': opttick,
        'symbol': symbol,
        'exp_date': exp_date,
        'strike': strike,
        'right': right,
        'rows': rows,
        'time_uncached': elapsed,
        'time_cached': None,
        'speedup': None
    })

print("\n" + "-" * 80)
print(f"PASS 1 TOTAL: {total_rows_uncached} rows in {total_time_uncached:.2f}s")
print(f"Average per ticker: {total_time_uncached/len(tickers):.2f}s")

# =============================================================================
# SECOND PASS: Cache HIT - Query all tickers again (cached)
# =============================================================================
print("\n" + "="*80)
print("PASS 2: CACHED QUERIES (Cache HIT)")
print("="*80)

total_time_cached = 0
total_rows_cached = 0

for i, opttick in enumerate(tickers, 1):
    print(f"\n[{i}/{len(tickers)}] {opttick}")
    print("-" * 60)
    
    # Parse ticker
    parsed = parse_option_tick(opttick)
    symbol = parsed['ticker']
    exp_date = parsed['exp_date']
    right = parsed['put_call']
    strike = parsed['strike']
    exp = pd.to_datetime(exp_date)
    
    # Same date logic as pass 1
    today = pd.Timestamp.now().normalize()
    if exp > today:
        start = today - BDay(60)
        end = today
    else:
        start = exp - BDay(60)
        end = exp
    
    # Create NEW manager (fresh instance)
    manager = CachedOptionDataManager(
        symbol=symbol,
        exp=exp_date,
        right=right,
        strike=strike
    )
    
    # Test GREEKS (should hit cache)
    start_time = time.time()
    result = manager.get_timeseries(start, end, interval='1d', type_='greeks', model='bs')
    elapsed = time.time() - start_time
    
    rows = len(result.post_processed_data)
    total_time_cached += elapsed
    total_rows_cached += rows
    
    # Calculate speedup
    time_uncached = results[i-1]['time_uncached']
    speedup = ((time_uncached - elapsed) / time_uncached) * 100
    improvement = time_uncached / elapsed
    
    print(f"  ‚úì Greeks: {rows} rows in {elapsed:.3f}s")
    print(f"  ‚ö° Speedup: {speedup:.1f}% ({improvement:.1f}x faster)")
    
    # Update result
    results[i-1]['time_cached'] = elapsed
    results[i-1]['speedup'] = speedup

print("\n" + "-" * 80)
print(f"PASS 2 TOTAL: {total_rows_cached} rows in {total_time_cached:.2f}s")
print(f"Average per ticker: {total_time_cached/len(tickers):.2f}s")

# =============================================================================
# SUMMARY
# =============================================================================
print("\n" + "="*80)
print("PERFORMANCE SUMMARY - ALL TICKERS")
print("="*80)

# Table header
print(f"\n{'Ticker':<20} {'Rows':<8} {'Uncached':<12} {'Cached':<12} {'Speedup':<12} {'Improvement'}")
print("-" * 90)

# Table rows
for r in results:
    improvement = r['time_uncached'] / r['time_cached']
    time_u = f"{r['time_uncached']:.3f}s"
    time_c = f"{r['time_cached']:.3f}s"
    speedup_str = f"{r['speedup']:.1f}%"
    improve_str = f"{improvement:.1f}x"
    print(f"{r['ticker']:<20} {r['rows']:<8} {time_u:<12} {time_c:<12} {speedup_str:<12} {improve_str}")

# Totals
print("-" * 90)
total_speedup = ((total_time_uncached - total_time_cached) / total_time_uncached) * 100
total_improvement = total_time_uncached / total_time_cached
time_u_total = f"{total_time_uncached:.2f}s"
time_c_total = f"{total_time_cached:.2f}s"
speedup_total_str = f"{total_speedup:.1f}%"
improve_total_str = f"{total_improvement:.1f}x"
print(f"{'TOTAL':<20} {total_rows_uncached:<8} {time_u_total:<12} {time_c_total:<12} {speedup_total_str:<12} {improve_total_str}")

# Statistics
print("\n" + "="*80)
print("STATISTICS")
print("="*80)

avg_uncached = total_time_uncached / len(tickers)
avg_cached = total_time_cached / len(tickers)
avg_speedup = sum(r['speedup'] for r in results) / len(results)
min_speedup = min(r['speedup'] for r in results)
max_speedup = max(r['speedup'] for r in results)

fastest_ticker = min(results, key=lambda x: x['time_cached'])
slowest_ticker = max(results, key=lambda x: x['time_cached'])

print(f"\nüìä Averages:")
print(f"  ‚Ä¢ Per ticker (uncached): {avg_uncached:.2f}s")
print(f"  ‚Ä¢ Per ticker (cached):   {avg_cached:.2f}s")
print(f"  ‚Ä¢ Average speedup:       {avg_speedup:.1f}%")
print(f"  ‚Ä¢ Speedup range:         {min_speedup:.1f}% - {max_speedup:.1f}%")

print(f"\n‚ö° Performance:")
print(f"  ‚Ä¢ Total time saved:      {total_time_uncached - total_time_cached:.2f}s")
print(f"  ‚Ä¢ Overall improvement:   {total_improvement:.1f}x faster")
print(f"  ‚Ä¢ Fastest cached query:  {fastest_ticker['ticker']} ({fastest_ticker['time_cached']:.3f}s)")
print(f"  ‚Ä¢ Slowest cached query:  {slowest_ticker['ticker']} ({slowest_ticker['time_cached']:.3f}s)")

print(f"\nüìà Data Processed:")
print(f"  ‚Ä¢ Total rows:            {total_rows_uncached}")
print(f"  ‚Ä¢ Tickers tested:        {len(tickers)}")
print(f"  ‚Ä¢ Date range per ticker: 60 business days")

print("\n" + "="*80)
print("KEY INSIGHTS")
print("="*80)
print(f"‚úì Tested {len(tickers)} option tickers across multiple underlyings")
print(f"‚úì Each ticker queried 60 business days of greeks data (includes spot + vol)")
print(f"‚úì Cache delivers {total_speedup:.1f}% speedup ({total_improvement:.1f}x faster)")
print(f"‚úì Total time saved: {total_time_uncached - total_time_cached:.1f}s across all tickers")
print(f"‚úì Average per-ticker speedup: {avg_speedup:.1f}%")
print(f"‚úì Cascade caching working: Greeks reuses cached Spot & Vol for each ticker")
print(f"‚úì Second queries almost instant (~{avg_cached:.2f}s avg vs ~{avg_uncached:.2f}s uncached)")

print("\n" + "="*80)
print("MULTI-TICKER PERFORMANCE TEST COMPLETE")
print("="*80)


# ============================================================================
# PARTIAL CACHE TESTS
# ============================================================================
if ENABLE_PARTIAL_CACHE_TEST:
    print("\n\n" + "="*80)
    print("PARTIAL CACHE TESTS")
    print("="*80)
    print("Testing scenarios where cache has SOME but not ALL requested dates")
    print("="*80)
    
    # Test 1: Query first half of date range, then query full range
    print("\nüìã TEST 1: Partial Date Range")
    print("-" * 80)
    test_ticker = tickers[0]  # Use first ticker
    print(f"Ticker: {test_ticker}")
    
    # Clear cache for this ticker
    spot_cache, bs_vol, bs_greeks, binom_vol, binom_greeks = load_option_data_cache()
    bs_greeks.clear()
    bs_vol.clear()
    spot_cache.clear()
    
    # Get expiration and calculate dates
    exp = get_expiration_date(test_ticker)
    today = pd.Timestamp.now().normalize()
    
    if exp > today:
        full_start = today - BDay(60)
        full_end = today
    else:
        full_start = exp - BDay(60)
        full_end = exp
    
    # Calculate partial range (first 30 days)
    partial_end = full_start + BDay(30)
    
    print(f"Full range:    {full_start.date()} to {full_end.date()} (60 days)")
    print(f"Partial range: {full_start.date()} to {partial_end.date()} (30 days)")
    
    # Step 1: Query partial range (cache first 30 days)
    print("\nStep 1: Querying partial range (30 days)...")
    dm_partial = CachedOptionDataManager(test_ticker)
    dm_partial.set_skip_mysql_query(True)
    
    start_partial = time.time()
    result_partial = dm_partial.get_timeseries(partial_end - BDay(30), partial_end, '1d', 'greeks', 'bs')
    time_partial = time.time() - start_partial
    print(f"‚úì Cached {len(result_partial)} rows in {time_partial:.3f}s")
    
    # Step 2: Query full range (should use cache for first 30, fetch remaining 30)
    print("\nStep 2: Querying full range (60 days)...")
    dm_full = CachedOptionDataManager(test_ticker)
    dm_full.set_skip_mysql_query(True)
    
    start_full = time.time()
    result_full = dm_full.get_timeseries(full_start, full_end, '1d', 'greeks', 'bs')
    time_full = time.time() - start_full
    print(f"‚úì Got {len(result_full)} rows in {time_full:.3f}s")
    print(f"  Expected: ~{len(result_partial)} from cache + ~{len(result_full) - len(result_partial)} new")
    
    # Verify data integrity
    partial_dates = set(result_partial.index)
    full_dates = set(result_full.index)
    
    if partial_dates.issubset(full_dates):
        print("‚úÖ PASS: Partial data is subset of full data")
    else:
        missing = partial_dates - full_dates
        print(f"‚ùå FAIL: {len(missing)} dates from partial query missing in full query")
        print(f"   Missing dates: {sorted(missing)}")
    
    if len(result_full) >= len(result_partial):
        print(f"‚úÖ PASS: Full query has more/equal rows ({len(result_full)} >= {len(result_partial)})")
    else:
        print(f"‚ùå FAIL: Full query has fewer rows ({len(result_full)} < {len(result_partial)})")
    
    print(f"\nPerformance: Full query with partial cache took {time_full:.3f}s")
    print(f"             (vs {time_partial:.3f}s for just partial range)")
    
    # Test 2: Gap in cached data
    print("\n\nüìã TEST 2: Cache with Gap")
    print("-" * 80)
    test_ticker2 = tickers[1] if len(tickers) > 1 else tickers[0]
    print(f"Ticker: {test_ticker2}")
    
    # Clear cache
    bs_greeks.clear()
    bs_vol.clear()
    spot_cache.clear()
    
    exp2 = get_expiration_date(test_ticker2)
    if exp2 > today:
        full_start2 = today - BDay(60)
        full_end2 = today
    else:
        full_start2 = exp2 - BDay(60)
        full_end2 = exp2
    
    # Cache first and last 20 days, creating a gap in the middle
    gap_start = full_start2 + BDay(20)
    gap_end = full_end2 - BDay(20)
    
    print(f"Full range:     {full_start2.date()} to {full_end2.date()} (60 days)")
    print(f"Cache segment 1: {full_start2.date()} to {gap_start.date()} (20 days)")
    print(f"GAP:            {gap_start.date()} to {gap_end.date()} (20 days)")
    print(f"Cache segment 2: {gap_end.date()} to {full_end2.date()} (20 days)")
    
    # Cache first 20 days
    print("\nStep 1: Caching first 20 days...")
    dm_seg1 = CachedOptionDataManager(test_ticker2)
    dm_seg1.set_skip_mysql_query(True)
    result_seg1 = dm_seg1.get_timeseries(full_start2, gap_start, '1d', 'greeks', 'bs')
    print(f"‚úì Cached {len(result_seg1)} rows")
    
    # Cache last 20 days
    print("\nStep 2: Caching last 20 days...")
    dm_seg2 = CachedOptionDataManager(test_ticker2)
    dm_seg2.set_skip_mysql_query(True)
    result_seg2 = dm_seg2.get_timeseries(gap_end, full_end2, '1d', 'greeks', 'bs')
    print(f"‚úì Cached {len(result_seg2)} rows")
    
    # Query full range (should detect gap and fill it)
    print("\nStep 3: Querying full range with gap...")
    dm_gap = CachedOptionDataManager(test_ticker2)
    dm_gap.set_skip_mysql_query(True)
    
    start_gap = time.time()
    result_gap = dm_gap.get_timeseries(full_start2, full_end2, '1d', 'greeks', 'bs')
    time_gap = time.time() - start_gap
    
    expected_rows = len(result_seg1) + len(result_seg2) + (gap_end - gap_start).days
    print(f"‚úì Got {len(result_gap)} rows in {time_gap:.3f}s")
    print(f"  Expected: ~{len(result_seg1)} (seg1) + gap data + ~{len(result_seg2)} (seg2)")
    
    # Verify no missing dates
    date_range = pd.date_range(full_start2, full_end2, freq='B')
    result_dates = set(result_gap.index.normalize())
    missing_dates = set(date_range) - result_dates
    
    if len(missing_dates) == 0:
        print("‚úÖ PASS: No missing dates, gap was filled")
    else:
        print(f"‚ö†Ô∏è WARNING: {len(missing_dates)} dates still missing")
        print(f"   Missing: {sorted(missing_dates)[:5]}...")
    
    if len(result_gap) >= len(result_seg1) + len(result_seg2):
        print(f"‚úÖ PASS: Full query has all cached data plus gap fill")
    else:
        print(f"‚ö†Ô∏è WARNING: Full query has fewer rows than expected")
    
    # Test 3: Overlapping cache updates
    print("\n\nüìã TEST 3: Overlapping Cache Updates")
    print("-" * 80)
    test_ticker3 = tickers[2] if len(tickers) > 2 else tickers[0]
    print(f"Ticker: {test_ticker3}")
    
    # Clear cache
    bs_greeks.clear()
    bs_vol.clear()
    spot_cache.clear()
    
    exp3 = get_expiration_date(test_ticker3)
    if exp3 > today:
        base_start = today - BDay(60)
        base_end = today
    else:
        base_start = exp3 - BDay(60)
        base_end = exp3
    
    overlap_mid = base_start + BDay(30)
    
    print(f"Query 1: {base_start.date()} to {overlap_mid.date()} (30 days)")
    print(f"Query 2: {(overlap_mid - BDay(10)).date()} to {base_end.date()} (40 days, 10-day overlap)")
    
    # First query
    print("\nQuery 1: Caching first 30 days...")
    dm_q1 = CachedOptionDataManager(test_ticker3)
    dm_q1.set_skip_mysql_query(True)
    result_q1 = dm_q1.get_timeseries(base_start, overlap_mid, '1d', 'greeks', 'bs')
    print(f"‚úì Cached {len(result_q1)} rows")
    
    # Second query with overlap
    print("\nQuery 2: Caching with 10-day overlap...")
    dm_q2 = CachedOptionDataManager(test_ticker3)
    dm_q2.set_skip_mysql_query(True)
    
    start_q2 = time.time()
    result_q2 = dm_q2.get_timeseries(overlap_mid - BDay(10), base_end, '1d', 'greeks', 'bs')
    time_q2 = time.time() - start_q2
    print(f"‚úì Got {len(result_q2)} rows in {time_q2:.3f}s")
    print(f"  Expected: ~10 from cache + ~30 new")
    
    # Verify no duplicate data
    if result_q2.index.is_unique:
        print("‚úÖ PASS: No duplicate dates in result")
    else:
        dupes = result_q2.index[result_q2.index.duplicated()].tolist()
        print(f"‚ùå FAIL: {len(dupes)} duplicate dates found: {dupes}")
    
    # Verify overlapping dates have consistent data
    overlap_dates = result_q1.index.intersection(result_q2.index)
    if len(overlap_dates) > 0:
        print(f"‚úì Found {len(overlap_dates)} overlapping dates")
        # Compare first column values for overlapping dates
        col = result_q1.columns[0]
        q1_overlap = result_q1.loc[overlap_dates, col]
        q2_overlap = result_q2.loc[overlap_dates, col]
        
        if (q1_overlap == q2_overlap).all():
            print("‚úÖ PASS: Overlapping data is identical")
        else:
            diffs = (q1_overlap != q2_overlap).sum()
            print(f"‚ö†Ô∏è WARNING: {diffs} differences in overlapping data")
    
    print("\n" + "="*80)
    print("PARTIAL CACHE TESTS COMPLETE")
    print("="*80)
    print("\n‚úì Test 1 (Partial Range): Cache expansion from 30 to 60 days")
    print("‚úì Test 2 (Gap Filling): Query with gap in cached data")
    print("‚úì Test 3 (Overlap): Overlapping cache updates handled correctly")
else:
    print("\n‚è≠Ô∏è  Partial cache tests SKIPPED (set ENABLE_PARTIAL_CACHE_TEST=True to enable)")

print("\n" + "="*80)
print("ALL TESTS COMPLETE")
print("="*80)
