{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "79ddd501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from trade.helpers.helper import CustomCache\n",
    "from trade.helpers.helper_types import SingletonMetaClass\n",
    "from trade.optionlib.config.defaults import DIVIDEND_LOOKBACK_YEARS, DIVIDEND_FORECAST_METHOD\n",
    "from trade.optionlib.config.types import DiscreteDivGrowthModel, DivType\n",
    "from trade.optionlib.assets.dividend import (\n",
    "    get_vectorized_dividend_scehdule,\n",
    "    vector_convert_to_time_frac,\n",
    "    vectorized_discrete_pv,\n",
    "    get_vectorized_dividend_rate,\n",
    "    get_vectorized_continuous_dividends,\n",
    "    get_vectorized_dividend_rate,\n",
    "    get_div_histories,\n",
    "    _dual_project_dividends,\n",
    "    ScheduleEntry\n",
    ")\n",
    "import os\n",
    "from EventDriven.riskmanager.market_data import MarketTimeseries\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Union, List\n",
    "from typing import overload, Literal\n",
    "DM_GEN_PATH = Path(os.getenv(\"GEN_CACHE_PATH\")) / \"dm_gen_cache\"\n",
    "TS = MarketTimeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ec207715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016-12-30         NaN\n",
       "2017-01-02         NaN\n",
       "2017-01-03    0.005323\n",
       "2017-01-04    0.005329\n",
       "2017-01-05    0.005302\n",
       "                ...   \n",
       "2025-12-31    0.000956\n",
       "2026-01-01         NaN\n",
       "2026-01-02    0.000959\n",
       "2026-01-05    0.000973\n",
       "2026-01-06    0.000991\n",
       "Length: 2353, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ff5684eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sym': 'AAPL',\n",
       " 'date': '2023-01-03',\n",
       " 'spot': open                 128.34378\n",
       " high                128.954561\n",
       " low                 122.324586\n",
       " close               123.211212\n",
       " volume               112117500\n",
       " chain_price         123.211212\n",
       " unadjusted_close    492.844849\n",
       " split_ratio                1.0\n",
       " cum_split                  4.0\n",
       " split_factor               1.0\n",
       " max_cum_split              4.0\n",
       " is_split_date            False\n",
       " Name: 2023-01-03 00:00:00, dtype: object,\n",
       " 'chain_spot': open                       128.34378\n",
       " high                      128.954561\n",
       " low                       122.324586\n",
       " close                     123.211212\n",
       " volume                     112117500\n",
       " chain_price               123.211212\n",
       " unadjusted_close        27599.311523\n",
       " split_ratio                      1.0\n",
       " cum_split                      224.0\n",
       " split_factor                     1.0\n",
       " max_cum_split                  224.0\n",
       " is_split_date                  False\n",
       " cum_split_from_start             4.0\n",
       " Name: 2023-01-03 00:00:00, dtype: object,\n",
       " 'rates': daily                       0.004559\n",
       " annualized                    0.0426\n",
       " name                            ^IRX\n",
       " description    13 WEEK TREASURY BILL\n",
       " Name: 2023-01-03 00:00:00, dtype: object,\n",
       " 'dividends': np.float64(0.23),\n",
       " 'dividend_yield': np.float64(0.0018667132314604627),\n",
       " 'additional': {}}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS.get_at_index(\"AAPL\", \"2023-01-03\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2107f515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aapl':                     amount\n",
       " ex_dividend_date          \n",
       " 1987-05-11        0.000536\n",
       " 1987-08-10        0.000536\n",
       " 1987-11-17        0.000714\n",
       " 1988-02-12        0.000714\n",
       " 1988-05-16        0.000714\n",
       " ...                    ...\n",
       " 2024-11-08        0.250000\n",
       " 2025-02-10        0.250000\n",
       " 2025-05-12        0.260000\n",
       " 2025-08-11        0.260000\n",
       " 2025-11-10        0.260000\n",
       " \n",
       " [89 rows x 1 columns]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = get_div_histories([\"aapl\"])\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "627126c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of trade.optionlib.assets.dividend failed: Traceback (most recent call last):\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 323, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: __class__ assignment: 'ScheduleEntry' object layout differs from 'ScheduleEntry'\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [], Timestamp('2026-11-11 00:00:00'))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dual_project_dividends(\n",
    "    valuation_date=\"2026-11-11\",\n",
    "    end_date=\"2026-12-31\",\n",
    "    div_history=hist['aapl'],\n",
    "    inferred_growth_rate=0.10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "813023d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2024, 2, 9), 0.24), (datetime.date(2024, 5, 10), 0.25), (datetime.date(2024, 8, 12), 0.25), (datetime.date(2024, 11, 8), 0.25), (datetime.date(2025, 2, 10), 0.25), (datetime.date(2025, 5, 12), 0.26), (datetime.date(2025, 8, 11), 0.26), (datetime.date(2025, 11, 10), 0.26), (datetime.date(2026, 2, 10), np.float64(0.26)), (datetime.date(2026, 5, 10), np.float64(0.26285714285714284)), (datetime.date(2026, 8, 10), np.float64(0.26574568288854006)), (datetime.date(2026, 11, 10), np.float64(0.26866596511808444))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of trade.optionlib.assets.dividend failed: Traceback (most recent call last):\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/Users/chiemelienwanisobi/miniconda3/envs/openbb_new_use/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 323, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: __class__ assignment: 'ScheduleEntry' object layout differs from 'ScheduleEntry'\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<ScheduleEntry: 2024-02-09 - 0.24>,\n",
       " <ScheduleEntry: 2024-05-10 - 0.25>,\n",
       " <ScheduleEntry: 2024-08-12 - 0.25>,\n",
       " <ScheduleEntry: 2024-11-08 - 0.25>,\n",
       " <ScheduleEntry: 2025-02-10 - 0.25>,\n",
       " <ScheduleEntry: 2025-05-12 - 0.26>,\n",
       " <ScheduleEntry: 2025-08-11 - 0.26>,\n",
       " <ScheduleEntry: 2025-11-10 - 0.26>,\n",
       " <ScheduleEntry: 2026-02-10 - 0.26>,\n",
       " <ScheduleEntry: 2026-05-10 - 0.26285714285714284>,\n",
       " <ScheduleEntry: 2026-08-10 - 0.26574568288854006>,\n",
       " <ScheduleEntry: 2026-11-10 - 0.26866596511808444>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_schedule = get_vectorized_dividend_scehdule(\n",
    "    tickers = [\"AAPL\"],\n",
    "    end_dates = [\"2026-12-31\"],\n",
    "    start_dates=[\"2024-01-01\"],\n",
    "    method=DiscreteDivGrowthModel.CONSTANT_AVG.value,\n",
    ")\n",
    "\n",
    "div_schedule[0].schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0524b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mget_vectorized_dividend_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mspots\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvaluation_dates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Get the vectorized dividend rate for a list of tickers based on their historical dividend data.\n",
      "\n",
      "tickers: str or List[str] - Ticker symbols of the stocks.\n",
      "spots: List[float] - Current spot prices for each ticker.\n",
      "valuation_dates: List[datetime] - Dates for which to calculate the dividend rates.\n",
      "\n",
      "Returns a numpy array of dividend rates.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/cloned_repos/QuantTools/trade/optionlib/assets/dividend.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "get_vectorized_dividend_rate(\n",
    "    t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cada1f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Schedule: 12 dividends>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac = vector_convert_to_time_frac(schedules=div_schedule, valuation_dates=[\"2024-01-01\"], end_dates=[\"2026-12-31\"])\n",
    "# pv = vectorized_discrete_pv(\n",
    "#     schedules=div_schedule,\n",
    "#     r = [0.05] * len(div_schedule),\n",
    "#     _valuation_dates=[\"2024-01-01\"],\n",
    "#     _end_dates=[\"2026-12-31\"]\n",
    "# )\n",
    "# pv\n",
    "\n",
    "frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe3aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.10677618069815195, 0.24),\n",
       " (0.35592060232717315, 0.25),\n",
       " (0.6132785763175906, 0.25),\n",
       " (0.8542094455852156, 0.25),\n",
       " (1.111567419575633, 0.25),\n",
       " (1.3607118412046544, 0.26),\n",
       " (1.6098562628336757, 0.26),\n",
       " (1.8590006844626967, 0.26),\n",
       " (2.11088295687885, np.float64(0.26)),\n",
       " (2.3545516769336072, np.float64(0.26285714285714284)),\n",
       " (2.6064339493497606, np.float64(0.26574568288854006)),\n",
       " (2.858316221765914, np.float64(0.26866596511808444))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac[0].schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87eb5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cache Key construction\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, date, time, timezone\n",
    "from enum import Enum\n",
    "from hashlib import sha1\n",
    "from typing import Any, Dict, Mapping, Optional, Tuple\n",
    "\n",
    "\n",
    "class Interval(str, Enum):\n",
    "    INTRADAY = \"intraday\"  # historical intraday timestamp\n",
    "    EOD = \"eod\"  # end-of-day daily snapshot\n",
    "    NA = \"na\"  # not applicable\n",
    "\n",
    "class SeriesId(str, Enum):\n",
    "    HIST = \"hist\"\n",
    "    AT_TIME = \"at_time\"\n",
    "    SNAPSHOT = \"snapshot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f6e98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactType(str, Enum):\n",
    "    # Market / inputs\n",
    "    SPOT = \"spot\"\n",
    "    CHAIN = \"chain\"\n",
    "    RATES = \"rates\"\n",
    "    DIVS = \"divs\"\n",
    "    FWD = \"forward\"\n",
    "\n",
    "    # Volatility\n",
    "    IV = \"iv\"\n",
    "    TVAR = \"tvar\"\n",
    "\n",
    "    # Greeks\n",
    "    GREEKS = \"greeks\"\n",
    "    DELTA = \"delta\"\n",
    "    GAMMA = \"gamma\"\n",
    "    VEGA = \"vega\"\n",
    "    THETA = \"theta\"\n",
    "    VOMMA = \"vomma\"\n",
    "    VANNA = \"vanna\"\n",
    "\n",
    "\n",
    "def _norm_str(x: str) -> str:\n",
    "    return x.strip().upper()\n",
    "\n",
    "def _safe_part(x: Optional[str]) -> str:\n",
    "    return x if x not in (None, \"\", \"None\") else \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3d6d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol:AAPL|interval:eod|artifact_type:iv|series_id:hist|date:20240101|model:SABR\n"
     ]
    }
   ],
   "source": [
    "def _format_value(v: Any) -> str:\n",
    "    \"\"\"\n",
    "    Keep it simple + deterministic.\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return \"-\"\n",
    "    if isinstance(v, Enum):\n",
    "        return str(v.value)\n",
    "    if isinstance(v, str):\n",
    "        return _norm_str(v)\n",
    "    if isinstance(v, bool):\n",
    "        return \"1\" if v else \"0\"\n",
    "    if isinstance(v, (int,)):\n",
    "        return str(v)\n",
    "    if isinstance(v, float):\n",
    "        # avoid 0.30000000000004 style keys\n",
    "        return f\"{v:.12g}\"\n",
    "    if isinstance(v, datetime):\n",
    "        # stable, compact. (no tz handling by design here)\n",
    "        return v.strftime(\"%Y%m%dT%H%M%S\")\n",
    "    if isinstance(v, date):\n",
    "        return v.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    if isinstance(v, time):\n",
    "        return v.strftime(\"%H%M%S\")\n",
    "    return str(v)\n",
    "\n",
    "\n",
    "def construct_cache_key(\n",
    "    symbol: str,\n",
    "    interval: Optional[Interval],\n",
    "    artifact_type: ArtifactType,\n",
    "    series_id: SeriesId,\n",
    "    **extra_parts: Any,\n",
    ") -> str:\n",
    "    \n",
    "    if series_id in (SeriesId.AT_TIME, SeriesId.SNAPSHOT):\n",
    "        assert 'time' in extra_parts, \"time must be provided for at_time or snapshot series_id\"\n",
    "        assert 'date' in extra_parts, \"date must be provided for at_time or snapshot series_id\"\n",
    "        assert isinstance(extra_parts['time'], time), \"time must be a time object\"\n",
    "        assert isinstance(extra_parts['date'], date), \"date must be a date object\"\n",
    "\n",
    "\n",
    "    parts = [\n",
    "        f\"symbol:{_norm_str(symbol)}\",\n",
    "        f\"interval:{_format_value(interval)}\",\n",
    "        f\"artifact_type:{artifact_type.value}\",\n",
    "        f\"series_id:{series_id.value}\"\n",
    "    ]\n",
    "\n",
    "    for k in sorted(extra_parts.keys()):\n",
    "        parts.append(f\"{k}:{_format_value(extra_parts[k])}\")\n",
    "\n",
    "    return \"|\".join(parts)\n",
    "\n",
    "\n",
    "k = construct_cache_key(\n",
    "    \"AAPL\",\n",
    "    Interval.EOD,\n",
    "    ArtifactType.IV,\n",
    "    SeriesId.HIST,\n",
    "    date=date(2024, 1, 1),\n",
    "    model=\"SABR\",\n",
    ")\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a81a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbol': 'AAPL', 'interval': 'eod', 'artifact_type': 'iv', 'series_id': 'hist', 'date': '20240101', 'model': 'SABR'}\n"
     ]
    }
   ],
   "source": [
    "def _parse_cache_key(key: str) -> Dict[str, str]:\n",
    "    parts = key.split(\"|\")\n",
    "    result = {}\n",
    "    for part in parts:\n",
    "        k, v = part.split(\":\", 1)\n",
    "        result[k] = v\n",
    "    return result\n",
    "\n",
    "print(_parse_cache_key(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d9ec5",
   "metadata": {},
   "source": [
    "## Building DataManagers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f19bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/chiemelienwanisobi/cloned_repos/QuantTools/.cache/dm_gen_cache',\n",
       " PosixPath('/Users/chiemelienwanisobi/cloned_repos/QuantTools/.cache/dm_gen_cache'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DM_GEN_PATH.as_posix(), DM_GEN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "608cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, ClassVar, Dict, Optional, Type, TypeVar\n",
    "\n",
    "# Assumes you already have these (from your cache_key module)\n",
    "# from cache_key import construct_cache_key, Interval, ArtifactType, SeriesId\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class CacheSpec:\n",
    "    \"\"\"\n",
    "    Optional: a small config object you can pass around, so all managers\n",
    "    initialize their caches in a consistent way.\n",
    "\n",
    "    If you already have a cache registry/factory, you may not need this.\n",
    "\n",
    "    args:\n",
    "        base_dir (Optional[Path]): Directory for cache storage.\n",
    "        default_expire_days (Optional[int]): Default expiration time in days. This is how many days till the entire cache entry expires.\n",
    "        default_expire_seconds (Optional[int]): Default expiration time in seconds. This is how many seconds till a single cache entry expires.\n",
    "        cache_fname (Optional[str]): Foldername for the cache storage.\n",
    "        clear_on_exit (bool): If True, clears the cache on exit.\n",
    "    \"\"\"\n",
    "\n",
    "    base_dir: Optional[Path] = DM_GEN_PATH.as_posix()\n",
    "    default_expire_days: Optional[int] = 500\n",
    "    default_expire_seconds: Optional[int] = None\n",
    "    cache_fname: Optional[str] = None\n",
    "    clear_on_exit: bool = False\n",
    "\n",
    "\n",
    "class BaseDataManager(ABC):\n",
    "    \"\"\"\n",
    "    Foundation class for all DataManagers.\n",
    "\n",
    "    Goals:\n",
    "    - Every inheritor gets a cache.\n",
    "    - Every inheritor MUST define CACHE_NAME.\n",
    "    - Provide consistent key creation (namespaced).\n",
    "    - Provide thin get/set/get_or_compute wrappers.\n",
    "    - Keep business logic out of the base.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- REQUIRED by inheritors ---\n",
    "    CACHE_NAME: ClassVar[str] = \"\"\n",
    "\n",
    "    # --- Optional defaults for common patterns ---\n",
    "    DEFAULT_INTERVAL: ClassVar[Optional[\"Interval\"]] = None\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"]  # prefer explicit in subclasses\n",
    "\n",
    "    # Internal registry to prevent accidental duplicate cache names\n",
    "    _CACHE_NAME_REGISTRY: ClassVar[Dict[str, Type[\"BaseDataManager\"]]] = {}\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs: Any) -> None:\n",
    "        super().__init_subclass__(**kwargs)\n",
    "\n",
    "        # Skip enforcement for the abstract base itself\n",
    "        if cls is BaseDataManager:\n",
    "            return\n",
    "\n",
    "        cache_name = getattr(cls, \"CACHE_NAME\", None)\n",
    "\n",
    "        if not isinstance(cache_name, str) or not cache_name.strip():\n",
    "            raise TypeError(f\"{cls.__name__} must define a non-empty class variable CACHE_NAME: str\")\n",
    "\n",
    "        cache_name = cache_name.strip()\n",
    "\n",
    "        # Enforce uniqueness to avoid collisions\n",
    "        existing = cls._CACHE_NAME_REGISTRY.get(cache_name)\n",
    "        # if existing is not None and existing is not cls:\n",
    "        #     raise TypeError(\n",
    "        #         f\"Duplicate CACHE_NAME='{cache_name}'. \"\n",
    "        #         f\"Already used by {existing.__name__}. \"\n",
    "        #         f\"Pick a unique CACHE_NAME for {cls.__name__}.\"\n",
    "        #     )\n",
    "\n",
    "        cls._CACHE_NAME_REGISTRY[cache_name] = cls\n",
    "\n",
    "        # Optional: enforce that DEFAULT_SERIES_ID exists (if you want)\n",
    "        if not hasattr(cls, \"DEFAULT_SERIES_ID\"):\n",
    "            raise TypeError(f\"{cls.__name__} must define DEFAULT_SERIES_ID (e.g., SeriesId.HIST).\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        cache_spec: Optional[CacheSpec] = None,\n",
    "        enable_namespacing: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache:\n",
    "            Your existing CustomCache instance (diskcache-backed).\n",
    "        cache_spec:\n",
    "            Optional shared configuration (base_dir, TTL defaults, etc.).\n",
    "        enable_namespacing:\n",
    "            If True, keys are prefixed with CACHE_NAME to avoid collisions.\n",
    "        \"\"\"\n",
    "        self.cache_spec = cache_spec or CacheSpec(cache_fname=self.CACHE_NAME)\n",
    "        self.cache = CustomCache(location=self.cache_spec.base_dir, \n",
    "                                 fname=self.cache_spec.cache_fname, \n",
    "                                 expire_days=self.cache_spec.default_expire_days,\n",
    "                                 clear_on_exit=self.cache_spec.clear_on_exit)\n",
    "        self.enable_namespacing = enable_namespacing\n",
    "\n",
    "    # Key construction\n",
    "    def make_key(\n",
    "        self,\n",
    "        *,\n",
    "        symbol: str,\n",
    "        interval: Optional[Interval] = None,\n",
    "        artifact_type: ArtifactType,\n",
    "        series_id: Optional[SeriesId] = None,\n",
    "        **extra_parts: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Namespaced key builder that wraps your construct_cache_key.\n",
    "\n",
    "        You decided:\n",
    "        - no caching SNAPSHOT series_id (but you might still request it)\n",
    "        - time is explicit if you do AT_TIME\n",
    "        \"\"\"\n",
    "        interval = interval if interval is not None else self.DEFAULT_INTERVAL\n",
    "        series_id = series_id if series_id is not None else self.DEFAULT_SERIES_ID\n",
    "\n",
    "        raw = construct_cache_key(\n",
    "            symbol=symbol,\n",
    "            interval=interval,\n",
    "            artifact_type=artifact_type,\n",
    "            series_id=series_id,\n",
    "            **extra_parts,\n",
    "        )\n",
    "\n",
    "        if not self.enable_namespacing:\n",
    "            return raw\n",
    "\n",
    "        return f\"{self.CACHE_NAME}|{raw}\"\n",
    "\n",
    "    # Cache IO\n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        return self.cache.get(key, default=default)\n",
    "\n",
    "    def set(self, key: str, value: Any, *, expire: Optional[int] = None) -> None:\n",
    "        if expire is None:\n",
    "            expire = self.cache_spec.default_expire_seconds\n",
    "        self.cache.set(key, value, expire=expire)\n",
    "\n",
    "    def delete(self, key: str) -> None:\n",
    "        self.cache.delete(key)\n",
    "\n",
    "    def contains(self, key: str) -> bool:\n",
    "        return key in self.cache\n",
    "    \n",
    "    def cache_it(self, key: str, value: Any, *, expire: Optional[int] = None) -> None:\n",
    "        raise NotImplementedError(f\"{self.__class__.__name__}.cache() not implemented.\")\n",
    "\n",
    "    def get_or_compute(\n",
    "        self,\n",
    "        key: str,\n",
    "        compute_fn: Callable[[], T],\n",
    "        *,\n",
    "        expire: Optional[int] = None,\n",
    "        force: bool = False,\n",
    "    ) -> T:\n",
    "        \"\"\"\n",
    "        Read-through caching helper.\n",
    "\n",
    "        force=True bypasses cache read, recomputes and overwrites cache.\n",
    "        \"\"\"\n",
    "        if not force:\n",
    "            hit = self.cache.get(key, default=None)\n",
    "            if hit is not None:\n",
    "                return hit  # type: ignore[return-value]\n",
    "\n",
    "        value = compute_fn()\n",
    "        self.set(key, value, expire=expire)\n",
    "        return value\n",
    "\n",
    "    # Offload hook (cron calls this)\n",
    "    def offload(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Optional standard hook.\n",
    "\n",
    "        You can override in subclasses or implement a shared offloader that\n",
    "        knows how to iterate keys / export values. Keeping it as a stub here\n",
    "        avoids forcing a storage design too early.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"{self.__class__.__name__}.offload() not implemented.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6e535",
   "metadata": {},
   "source": [
    "### Dividends DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b6bce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DividendsConfig(metaclass=SingletonMetaClass):\n",
    "    default_lookback_years: int = DIVIDEND_LOOKBACK_YEARS\n",
    "    default_forecast_method: DiscreteDivGrowthModel = DiscreteDivGrowthModel.CONSTANT\n",
    "    dividend_type: DivType = DivType.DISCRETE\n",
    "    include_special_dividends: bool = False\n",
    "\n",
    "    def assert_valid(self) -> None:\n",
    "        assert self.default_lookback_years > 0, \"Lookback years must be positive.\"\n",
    "        assert self.default_lookback_years <= 5, \"Lookback years seems too large. Max 5.\"\n",
    "        assert isinstance(self.default_forecast_method, DiscreteDivGrowthModel), \"Invalid forecast method. Expected DiscreteDivGrowthModel Enum.\"\n",
    "        assert isinstance(self.dividend_type, DivType), \"Invalid dividend type. Expected DivType Enum.\"\n",
    "        assert isinstance(self.include_special_dividends, bool), \"include_special_dividends must be a boolean.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.assert_valid()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        super().__setattr__(name, value)\n",
    "        self.assert_valid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "12698670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 2), (2, 3)}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(1,2), (2,3), (1,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "83047047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How dividends timeseries will work:\n",
    "## If discrete:\n",
    "    ## All constant(+...)  will cache up to < today\n",
    "    ## All None Constant will not cache\n",
    "## If continuous:\n",
    "    ## Rely on MarktetTimeseries to provide continuous dividend yield history. It already caches.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4fee6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DividendDataManager(BaseDataManager):\n",
    "    CACHE_NAME: ClassVar[str] = \"dividend_data_manager\"\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"] = SeriesId.HIST\n",
    "    CONFIG = DividendsConfig()\n",
    "    \n",
    "    def __init__(self, symbol: str, *, cache_spec: Optional[CacheSpec] = None, enable_namespacing: bool = False) -> None:\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def cache_it(self, key: str, value: Any, *, expire: Optional[int] = None, _type: str = \"discrete\") -> None:\n",
    "\n",
    "        \n",
    "        ## If discrete dividends, we first check if key exists\n",
    "        ## If it does, we add to it. Only values <= today.\n",
    "        ## If it does not, we create new entry\n",
    "        if _type == \"discrete\":\n",
    "            existing = self.get(key, default=None)\n",
    "            allowed = [entry for entry in value if entry.date <= datetime.now().date()]\n",
    "            if existing is not None:\n",
    "                # Merge existing and new values. We're expecting lists of ScheduleEntry\n",
    "                merged = existing + allowed\n",
    "                \n",
    "                ## Unique by date\n",
    "                uniques = list({entry.date: entry for entry in merged}.values())\n",
    "                self.set(key, uniques, expire=expire)\n",
    "                return\n",
    "            else:\n",
    "                self.set(key, allowed, expire=expire)\n",
    "                return\n",
    "\n",
    "        # For other types or if no existing, just setattr\n",
    "        self.set(key, value, expire=expire)\n",
    "\n",
    "    @overload\n",
    "    def get_data(self, \n",
    "                start_date: Union[datetime, str],\n",
    "                end_date: Union[datetime, str],\n",
    "                div_type: Optional[DivType] = None,\n",
    "                return_key: Literal[True] = True\n",
    "                ) -> tuple[pd.Series | List[ScheduleEntry], str]: ...\n",
    "\n",
    "    @overload\n",
    "    def get_data(self, \n",
    "                start_date: Union[datetime, str],\n",
    "                end_date: Union[datetime, str],\n",
    "                div_type: Optional[DivType] = None,\n",
    "                return_key: Literal[False] = False\n",
    "                ) -> pd.Series | List[ScheduleEntry]: ...\n",
    "\n",
    "    def get_data(self, \n",
    "                 start_date: Union[datetime, str],\n",
    "                 end_date: Union[datetime, str],\n",
    "                 div_type: Optional[DivType] = None,\n",
    "                 return_key: bool = False\n",
    "                 ) -> pd.Series | List[ScheduleEntry] | tuple[pd.Series | List[ScheduleEntry], str]:\n",
    "        \"\"\"\n",
    "        Get dividend data for the symbol. Ensures internal caching logic is respected.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : Union[datetime, str]\n",
    "            Start date for dividend data retrieval.\n",
    "        end_date : Union[datetime, str]\n",
    "            End date for dividend data retrieval.\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series | List[ScheduleEntry]\n",
    "            Dividend data in the appropriate format.\n",
    "        \"\"\"\n",
    "        div_type = DivType(div_type) if div_type is not None else self.CONFIG.dividend_type\n",
    "        if div_type == DivType.DISCRETE:\n",
    "            data, key = self.get_discrete_dividend_schedule(\n",
    "                ticker=self.symbol,\n",
    "                start_date=start_date.strftime(\"%Y-%m-%d\") if isinstance(start_date, datetime) else start_date,\n",
    "                end_date=end_date.strftime(\"%Y-%m-%d\") if isinstance(end_date, datetime) else end_date,\n",
    "            )\n",
    "        elif div_type == DivType.CONTINUOUS:\n",
    "             data = self.get_div_yield_history(self.symbol)\n",
    "             key = None        \n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dividend type: {div_type}\")\n",
    "        \n",
    "        if return_key:\n",
    "            return data, key\n",
    "        return data \n",
    "\n",
    "        \n",
    "\n",
    "    def get_discrete_dividend_schedule(\n",
    "        self,\n",
    "        *,\n",
    "        ticker: str,\n",
    "        end_date: str,\n",
    "        start_date: str,\n",
    "        valuation_date: Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "\n",
    "        method = self.CONFIG.default_forecast_method.value\n",
    "        lookback_years = self.CONFIG.default_lookback_years\n",
    "        key = self.make_key(\n",
    "            symbol=ticker,\n",
    "            artifact_type=ArtifactType.DIVS,\n",
    "            series_id=SeriesId.HIST,\n",
    "            method=method,\n",
    "            lookback_years=lookback_years,\n",
    "            current_state=\"schedule\",\n",
    "            interval=Interval.NA,\n",
    "        )\n",
    "\n",
    "        available_schedule = self.get(key, default=None)\n",
    "        if available_schedule is not None:\n",
    "            \n",
    "            ## If max date in available schedule >= end_date, we can use cache\n",
    "            max_cached_date = max(entry.date for entry in available_schedule)\n",
    "            min_cached_date = min(entry.date for entry in available_schedule)\n",
    "            fully_covered = (min_cached_date <= datetime.strptime(start_date, \"%Y-%m-%d\").date()) and (max_cached_date >= datetime.strptime(end_date, \"%Y-%m-%d\").date())\n",
    "            if fully_covered:\n",
    "                print(\"Cache fully covers requested date range.\")\n",
    "                \n",
    "                ## Filter to requested date range\n",
    "                filtered_schedule = [entry for entry in available_schedule if start_date <= entry.date.strftime(\"%Y-%m-%d\") <= end_date]\n",
    "                return filtered_schedule, key\n",
    "            else:\n",
    "                print(\"Cache partially covers requested date range. Fetching missing data.\")\n",
    "        \n",
    "        schedule = get_vectorized_dividend_scehdule(\n",
    "            tickers=[ticker],\n",
    "            end_dates=[end_date],\n",
    "            start_dates=[start_date],\n",
    "            method=method,\n",
    "            lookback_years=lookback_years,\n",
    "            valuation_dates=[valuation_date] if valuation_date else None,\n",
    "        )\n",
    "        raw_schedule = schedule[0].schedule\n",
    "        self.cache_it(key, raw_schedule, _type=\"discrete\")\n",
    "        \n",
    "        return raw_schedule, key\n",
    "    \n",
    "    def get_div_yield_history(self, symbol: str) -> pd.Series:\n",
    "        div_history = TS.get_timeseries(symbol)\n",
    "        return div_history.dividend_yield\n",
    "\n",
    "    def offload(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Example implementation of offload for DividendDataManager.\n",
    "        \"\"\"\n",
    "        print(f\"No offload logic implemented for {self.CACHE_NAME}\")\n",
    "\n",
    "test = DividendDataManager(symbol=\"AAPL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1eef60d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache fully covers requested date range.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<ScheduleEntry: 2019-02-08 - 0.1825>,\n",
       " <ScheduleEntry: 2025-02-10 - 0.25>,\n",
       " <ScheduleEntry: 2023-05-12 - 0.24>,\n",
       " <ScheduleEntry: 2017-11-10 - 0.1575>,\n",
       " <ScheduleEntry: 2015-11-05 - 0.13>,\n",
       " <ScheduleEntry: 2017-08-10 - 0.1575>,\n",
       " <ScheduleEntry: 2019-11-07 - 0.1925>,\n",
       " <ScheduleEntry: 2022-05-06 - 0.23>,\n",
       " <ScheduleEntry: 2018-11-08 - 0.1825>,\n",
       " <ScheduleEntry: 2021-05-07 - 0.22>,\n",
       " <ScheduleEntry: 2018-02-09 - 0.1575>,\n",
       " <ScheduleEntry: 2023-08-11 - 0.24>,\n",
       " <ScheduleEntry: 2024-05-10 - 0.25>,\n",
       " <ScheduleEntry: 2015-08-06 - 0.13>,\n",
       " <ScheduleEntry: 2017-02-09 - 0.1425>,\n",
       " <ScheduleEntry: 2013-08-08 - 0.108929>,\n",
       " <ScheduleEntry: 2021-08-06 - 0.22>,\n",
       " <ScheduleEntry: 2018-05-11 - 0.1825>,\n",
       " <ScheduleEntry: 2014-05-08 - 0.1175>,\n",
       " <ScheduleEntry: 2016-05-05 - 0.1425>,\n",
       " <ScheduleEntry: 2021-02-05 - 0.205>,\n",
       " <ScheduleEntry: 2018-08-10 - 0.1825>,\n",
       " <ScheduleEntry: 2016-11-03 - 0.1425>,\n",
       " <ScheduleEntry: 2013-11-06 - 0.108929>,\n",
       " <ScheduleEntry: 2020-11-06 - 0.205>,\n",
       " <ScheduleEntry: 2015-02-05 - 0.1175>,\n",
       " <ScheduleEntry: 2024-08-12 - 0.25>,\n",
       " <ScheduleEntry: 2017-05-11 - 0.1575>,\n",
       " <ScheduleEntry: 2016-08-04 - 0.1425>,\n",
       " <ScheduleEntry: 2015-05-07 - 0.13>,\n",
       " <ScheduleEntry: 2013-05-09 - 0.108929>,\n",
       " <ScheduleEntry: 2020-02-07 - 0.1925>,\n",
       " <ScheduleEntry: 2014-08-07 - 0.1175>,\n",
       " <ScheduleEntry: 2022-02-04 - 0.22>,\n",
       " <ScheduleEntry: 2022-08-05 - 0.23>,\n",
       " <ScheduleEntry: 2016-02-04 - 0.13>,\n",
       " <ScheduleEntry: 2020-05-08 - 0.205>,\n",
       " <ScheduleEntry: 2024-11-08 - 0.25>,\n",
       " <ScheduleEntry: 2025-08-11 - 0.26>,\n",
       " <ScheduleEntry: 2012-11-07 - 0.094643>,\n",
       " <ScheduleEntry: 2014-11-06 - 0.1175>,\n",
       " <ScheduleEntry: 2023-02-10 - 0.23>,\n",
       " <ScheduleEntry: 2019-08-09 - 0.1925>,\n",
       " <ScheduleEntry: 2025-05-12 - 0.26>,\n",
       " <ScheduleEntry: 2020-08-07 - 0.205>,\n",
       " <ScheduleEntry: 2021-11-05 - 0.22>,\n",
       " <ScheduleEntry: 2019-05-10 - 0.1925>,\n",
       " <ScheduleEntry: 2013-02-07 - 0.094643>,\n",
       " <ScheduleEntry: 2014-02-06 - 0.108929>,\n",
       " <ScheduleEntry: 2023-11-10 - 0.24>,\n",
       " <ScheduleEntry: 2022-11-04 - 0.23>,\n",
       " <ScheduleEntry: 2024-02-09 - 0.24>]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.CONFIG.dividend_type = DivType.DISCRETE\n",
    "test.get_data(\n",
    "    start_date=\"2012-10-08\",\n",
    "    end_date=\"2025-10-31\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb_new_use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
