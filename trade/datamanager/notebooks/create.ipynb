{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79ddd501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2026-01-18 19:43:31 [test] trade.__init__ INFO: Signal function for `_on_exit_sanitize` added to signal number 15.\n",
      "2026-01-18 19:43:31 [test] trade.__init__ INFO: Exit handler `_on_exit_sanitize` registered for normal program exit.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "from trade.helpers.helper import CustomCache\n",
    "from trade.helpers.helper_types import SingletonMetaClass\n",
    "from trade.optionlib.config.defaults import DIVIDEND_LOOKBACK_YEARS, DIVIDEND_FORECAST_METHOD\n",
    "from trade.optionlib.config.types import DiscreteDivGrowthModel, DivType\n",
    "from trade.optionlib.assets.dividend import (\n",
    "    get_vectorized_dividend_scehdule,\n",
    "    vector_convert_to_time_frac,\n",
    "    vectorized_discrete_pv,\n",
    "    get_vectorized_dividend_rate,\n",
    "    get_vectorized_continuous_dividends,\n",
    "    get_vectorized_dividend_rate,\n",
    "    get_div_histories,\n",
    "    _dual_project_dividends,\n",
    "    ScheduleEntry,\n",
    "    Schedule,\n",
    "    SECONDS_IN_YEAR,\n",
    "    SECONDS_IN_DAY,\n",
    ")\n",
    "import os\n",
    "from trade import HOLIDAY_SET\n",
    "from trade.helpers.helper import is_USholiday, is_busday, to_datetime\n",
    "from EventDriven.riskmanager.market_data import MarketTimeseries, AtIndexResult, TimeseriesData\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Union, List, Iterable\n",
    "from typing import overload, Literal\n",
    "from trade.helpers.helper import compare_dates, get_missing_dates\n",
    "from trade.helpers.Logging import setup_logger\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import BDay\n",
    "from trade.optionlib.config.defaults import OPTION_TIMESERIES_START_DATE\n",
    "from trade.helpers.decorators import cProfiler\n",
    "from trade.helpers.helper import print_top_cprofile_stats\n",
    "from trade.optionlib.assets.forward import (\n",
    "    vectorized_discrete_pv,\n",
    "    vectorized_forward_continuous,\n",
    "    vectorized_forward_discrete,\n",
    "    get_vectorized_continuous_dividends,\n",
    ")\n",
    "\n",
    "from trade.optionlib.vol.implied_vol import (\n",
    "    vector_vol_estimation,\n",
    "    bsm_vol_est_brute_force,\n",
    "    estimate_crr_implied_volatility,\n",
    "    crr_binomial_pricing\n",
    ")\n",
    "\n",
    "from trade.optionlib.pricing.binomial import vector_crr_binomial_pricing\n",
    "\n",
    "from trade.optionlib.utils.batch_operation import vector_batch_processor\n",
    "from trade.assets.rates import get_risk_free_rate_helper, _fetch_rates\n",
    "import time\n",
    "# from dbase.DataAPI.ThetaData import (\n",
    "#     retrieve_bulk_eod,\n",
    "#     retrieve_eod_ohlc,\n",
    "#     retrieve_quote,\n",
    "#     list_contracts,\n",
    "# )\n",
    "\n",
    "from dbase.DataAPI.ThetaData.v2 import (\n",
    "    retrieve_bulk_eod,\n",
    "    retrieve_eod_ohlc,\n",
    "    retrieve_quote,\n",
    "    list_contracts,\n",
    "    quote_to_eod_patch,\n",
    "    list_dates,\n",
    ")\n",
    "\n",
    "from trade.optionlib.utils.format import (\n",
    "    assert_equal_length, \n",
    "    convert_to_array\n",
    ")\n",
    "\n",
    "\n",
    "from dbase.DataAPI.ThetaData.utils import _handle_opttick_param\n",
    "from dbase.utils import default_timestamp\n",
    "logger = setup_logger(__name__, stream_log_level=\"INFO\")\n",
    "DM_GEN_PATH = Path(os.getenv(\"GEN_CACHE_PATH\")) / \"dm_gen_cache\"\n",
    "TS = MarketTimeseries(_end=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87eb5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cache Key construction\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, date, time, timezone\n",
    "from enum import Enum\n",
    "from hashlib import sha1\n",
    "from typing import Any, Dict, Mapping, Optional, Tuple\n",
    "\n",
    "\n",
    "class Interval(str, Enum):\n",
    "    INTRADAY = \"intraday\"  # historical intraday timestamp\n",
    "    EOD = \"eod\"  # end-of-day daily snapshot\n",
    "    NA = \"na\"  # not applicable\n",
    "\n",
    "class SeriesId(str, Enum):\n",
    "    HIST = \"hist\"\n",
    "    AT_TIME = \"at_time\"\n",
    "    SNAPSHOT = \"snapshot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e98be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtifactType(str, Enum):\n",
    "    # Market / inputs\n",
    "    SPOT = \"spot\"\n",
    "    CHAIN = \"chain\"\n",
    "    RATES = \"rates\"\n",
    "    DIVS = \"divs\"\n",
    "    FWD = \"forward\"\n",
    "    OPTION_SPOT = \"option_spot\"\n",
    "    DATES = \"dates\"\n",
    "\n",
    "    # Volatility\n",
    "    IV = \"iv\"\n",
    "    TVAR = \"tvar\"\n",
    "\n",
    "    # Greeks\n",
    "    GREEKS = \"greeks\"\n",
    "    DELTA = \"delta\"\n",
    "    GAMMA = \"gamma\"\n",
    "    VEGA = \"vega\"\n",
    "    THETA = \"theta\"\n",
    "    VOMMA = \"vomma\"\n",
    "    VANNA = \"vanna\"\n",
    "    RHO = \"rho\"\n",
    "\n",
    "\n",
    "def _norm_str(x: str) -> str:\n",
    "    return x.strip().upper()\n",
    "\n",
    "def _safe_part(x: Optional[str]) -> str:\n",
    "    return x if x not in (None, \"\", \"None\") else \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3d6d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol:AAPL|interval:eod|artifact_type:iv|series_id:hist|date:20240101|model:SABR\n"
     ]
    }
   ],
   "source": [
    "def _format_value(v: Any) -> str:\n",
    "    \"\"\"\n",
    "    Keep it simple + deterministic.\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return \"-\"\n",
    "    if isinstance(v, Enum):\n",
    "        return str(v.value)\n",
    "    if isinstance(v, str):\n",
    "        return _norm_str(v)\n",
    "    if isinstance(v, bool):\n",
    "        return \"1\" if v else \"0\"\n",
    "    if isinstance(v, (int,)):\n",
    "        return str(v)\n",
    "    if isinstance(v, float):\n",
    "        # avoid 0.30000000000004 style keys\n",
    "        return f\"{v:.12g}\"\n",
    "    if isinstance(v, datetime):\n",
    "        # stable, compact. (no tz handling by design here)\n",
    "        return v.strftime(\"%Y%m%dT%H%M%S\")\n",
    "    if isinstance(v, date):\n",
    "        return v.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    if isinstance(v, time):\n",
    "        return v.strftime(\"%H%M%S\")\n",
    "    return str(v)\n",
    "\n",
    "\n",
    "def construct_cache_key(\n",
    "    symbol: str,\n",
    "    interval: Optional[Interval],\n",
    "    artifact_type: ArtifactType,\n",
    "    series_id: SeriesId,\n",
    "    **extra_parts: Any,\n",
    ") -> str:\n",
    "    \"\"\"Constructs deterministic cache key from symbol, interval, artifact type, series ID, and extra parts.\"\"\"\n",
    "    \n",
    "    if series_id in (SeriesId.AT_TIME, SeriesId.SNAPSHOT):\n",
    "        assert 'time' in extra_parts, \"time must be provided for at_time or snapshot series_id\"\n",
    "        assert 'date' in extra_parts, \"date must be provided for at_time or snapshot series_id\"\n",
    "        assert isinstance(extra_parts['time'], time), \"time must be a time object\"\n",
    "        assert isinstance(extra_parts['date'], date), \"date must be a date object\"\n",
    "\n",
    "\n",
    "    parts = [\n",
    "        f\"symbol:{_norm_str(symbol)}\",\n",
    "        f\"interval:{_format_value(interval)}\",\n",
    "        f\"artifact_type:{artifact_type.value}\",\n",
    "        f\"series_id:{series_id.value}\"\n",
    "    ]\n",
    "\n",
    "    for k in sorted(extra_parts.keys()):\n",
    "        parts.append(f\"{k}:{_format_value(extra_parts[k])}\")\n",
    "\n",
    "    return \"|\".join(parts)\n",
    "\n",
    "\n",
    "k = construct_cache_key(\n",
    "    \"AAPL\",\n",
    "    Interval.EOD,\n",
    "    ArtifactType.IV,\n",
    "    SeriesId.HIST,\n",
    "    date=date(2024, 1, 1),\n",
    "    model=\"SABR\",\n",
    ")\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a81a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbol': 'AAPL', 'interval': 'eod', 'artifact_type': 'iv', 'series_id': 'hist', 'date': '20240101', 'model': 'SABR'}\n"
     ]
    }
   ],
   "source": [
    "def _parse_cache_key(key: str) -> Dict[str, str]:\n",
    "    \"\"\"Parses a pipe-delimited cache key into a dictionary of key-value pairs.\"\"\"\n",
    "    parts = key.split(\"|\")\n",
    "    result = {}\n",
    "    for part in parts:\n",
    "        k, v = part.split(\":\", 1)\n",
    "        result[k] = v\n",
    "    return result\n",
    "\n",
    "print(_parse_cache_key(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d9ec5",
   "metadata": {},
   "source": [
    "## Building DataManagers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38f19bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/chiemelienwanisobi/cloned_repos/QuantTools/.cache/dm_gen_cache',\n",
       " PosixPath('/Users/chiemelienwanisobi/cloned_repos/QuantTools/.cache/dm_gen_cache'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DM_GEN_PATH.as_posix(), DM_GEN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "608cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from abc import ABC\n",
    "from typing import Any, Callable, ClassVar, Dict, Optional, Type, TypeVar\n",
    "\n",
    "# Assumes you already have these (from your cache_key module)\n",
    "# from cache_key import construct_cache_key, Interval, ArtifactType, SeriesId\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "# REMEBER: Take out the commented out parts\n",
    "@dataclass(frozen=True, slots=True)\n",
    "class CacheSpec:\n",
    "    \"\"\"\n",
    "    Optional: a small config object you can pass around, so all managers\n",
    "    initialize their caches in a consistent way.\n",
    "\n",
    "    If you already have a cache registry/factory, you may not need this.\n",
    "\n",
    "    args:\n",
    "        base_dir (Optional[Path]): Directory for cache storage.\n",
    "        default_expire_days (Optional[int]): Default expiration time in days. This is how many days till the entire cache entry expires.\n",
    "        default_expire_seconds (Optional[int]): Default expiration time in seconds. This is how many seconds till a single cache entry expires.\n",
    "        cache_fname (Optional[str]): Foldername for the cache storage.\n",
    "        clear_on_exit (bool): If True, clears the cache on exit.\n",
    "    \"\"\"\n",
    "\n",
    "    base_dir: Optional[Path] = DM_GEN_PATH.as_posix()\n",
    "    default_expire_days: Optional[int] = 500\n",
    "    default_expire_seconds: Optional[int] = None\n",
    "    cache_fname: Optional[str] = None\n",
    "    clear_on_exit: bool = False\n",
    "\n",
    "\n",
    "class BaseDataManager(ABC):\n",
    "    \"\"\"\n",
    "    Foundation class for all DataManagers.\n",
    "\n",
    "    Goals:\n",
    "    - Every inheritor gets a cache.\n",
    "    - Every inheritor MUST define CACHE_NAME.\n",
    "    - Provide consistent key creation (namespaced).\n",
    "    - Provide thin get/set/get_or_compute wrappers.\n",
    "    - Keep business logic out of the base.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- REQUIRED by inheritors ---\n",
    "    CACHE_NAME: ClassVar[str] = \"\"\n",
    "\n",
    "    # --- Optional defaults for common patterns ---\n",
    "    DEFAULT_INTERVAL: ClassVar[Optional[\"Interval\"]] = None\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"]  # prefer explicit in subclasses\n",
    "\n",
    "    # Internal registry to prevent accidental duplicate cache names\n",
    "    _CACHE_NAME_REGISTRY: ClassVar[Dict[str, Type[\"BaseDataManager\"]]] = {}\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs: Any) -> None:\n",
    "        \"\"\"Enforces that all subclasses define CACHE_NAME and DEFAULT_SERIES_ID.\"\"\"\n",
    "        super().__init_subclass__(**kwargs)\n",
    "\n",
    "        # Skip enforcement for the abstract base itself\n",
    "        if cls is BaseDataManager:\n",
    "            return\n",
    "\n",
    "        cache_name = getattr(cls, \"CACHE_NAME\", None)\n",
    "\n",
    "        if not isinstance(cache_name, str) or not cache_name.strip():\n",
    "            raise TypeError(f\"{cls.__name__} must define a non-empty class variable CACHE_NAME: str\")\n",
    "\n",
    "        cache_name = cache_name.strip()\n",
    "\n",
    "        # Enforce uniqueness to avoid collisions\n",
    "        existing = cls._CACHE_NAME_REGISTRY.get(cache_name)\n",
    "        # if existing is not None and existing is not cls:\n",
    "        #     raise TypeError(\n",
    "        #         f\"Duplicate CACHE_NAME='{cache_name}'. \"\n",
    "        #         f\"Already used by {existing.__name__}. \"\n",
    "        #         f\"Pick a unique CACHE_NAME for {cls.__name__}.\"\n",
    "        #     )\n",
    "\n",
    "        cls._CACHE_NAME_REGISTRY[cache_name] = cls\n",
    "\n",
    "        # Optional: enforce that DEFAULT_SERIES_ID exists (if you want)\n",
    "        if not hasattr(cls, \"DEFAULT_SERIES_ID\"):\n",
    "            raise TypeError(f\"{cls.__name__} must define DEFAULT_SERIES_ID (e.g., SeriesId.HIST).\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        cache_spec: Optional[CacheSpec] = None,\n",
    "        enable_namespacing: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        cache:\n",
    "            Your existing CustomCache instance (diskcache-backed).\n",
    "        cache_spec:\n",
    "            Optional shared configuration (base_dir, TTL defaults, etc.).\n",
    "        enable_namespacing:\n",
    "            If True, keys are prefixed with CACHE_NAME to avoid collisions.\n",
    "        \"\"\"\n",
    "        self.cache_spec = cache_spec or CacheSpec(cache_fname=self.CACHE_NAME)\n",
    "        self.cache = CustomCache(location=self.cache_spec.base_dir, \n",
    "                                 fname=self.cache_spec.cache_fname, \n",
    "                                 expire_days=self.cache_spec.default_expire_days,\n",
    "                                 clear_on_exit=self.cache_spec.clear_on_exit)\n",
    "        self.enable_namespacing = enable_namespacing\n",
    "        out = self.cache.expire()\n",
    "        if out > 0:\n",
    "            logger.info(f\"{self.CACHE_NAME} has expired {out} entries\")\n",
    "\n",
    "    # Key construction\n",
    "    def make_key(\n",
    "        self,\n",
    "        *,\n",
    "        symbol: str,\n",
    "        interval: Optional[Interval] = None,\n",
    "        artifact_type: ArtifactType,\n",
    "        series_id: Optional[SeriesId] = None,\n",
    "        **extra_parts: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Namespaced key builder that wraps your construct_cache_key.\n",
    "\n",
    "        You decided:\n",
    "        - no caching SNAPSHOT series_id (but you might still request it)\n",
    "        - time is explicit if you do AT_TIME\n",
    "        \"\"\"\n",
    "        interval = interval if interval is not None else self.DEFAULT_INTERVAL\n",
    "        series_id = series_id if series_id is not None else self.DEFAULT_SERIES_ID\n",
    "\n",
    "        raw = construct_cache_key(\n",
    "            symbol=symbol,\n",
    "            interval=interval,\n",
    "            artifact_type=artifact_type,\n",
    "            series_id=series_id,\n",
    "            **extra_parts,\n",
    "        )\n",
    "\n",
    "        if not self.enable_namespacing:\n",
    "            return raw\n",
    "\n",
    "        return f\"{self.CACHE_NAME}|{raw}\"\n",
    "\n",
    "    # Cache IO\n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        return self.cache.get(key, default=default)\n",
    "\n",
    "    def set(self, key: str, value: Any, *, expire: Optional[int] = None) -> None:\n",
    "        if expire is None:\n",
    "            expire = self.cache_spec.default_expire_seconds\n",
    "        self.cache.set(key, value, expire=expire)\n",
    "\n",
    "    def delete(self, key: str) -> None:\n",
    "        self.cache.delete(key)\n",
    "\n",
    "    def contains(self, key: str) -> bool:\n",
    "        return key in self.cache\n",
    "    \n",
    "    def cache_it(self, key: str, value: Any, *, expire: Optional[int] = None) -> None:\n",
    "        raise NotImplementedError(f\"{self.__class__.__name__}.cache() not implemented.\")\n",
    "\n",
    "    def get_or_compute(\n",
    "        self,\n",
    "        key: str,\n",
    "        compute_fn: Callable[[], T],\n",
    "        *,\n",
    "        expire: Optional[int] = None,\n",
    "        force: bool = False,\n",
    "    ) -> T:\n",
    "        \"\"\"\n",
    "        Read-through caching helper.\n",
    "\n",
    "        force=True bypasses cache read, recomputes and overwrites cache.\n",
    "        \"\"\"\n",
    "        if not force:\n",
    "            hit = self.cache.get(key, default=None)\n",
    "            if hit is not None:\n",
    "                return hit  # type: ignore[return-value]\n",
    "\n",
    "        value = compute_fn()\n",
    "        self.set(key, value, expire=expire)\n",
    "        return value\n",
    "\n",
    "    # Offload hook (cron calls this)\n",
    "    def offload(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Optional standard hook.\n",
    "\n",
    "        You can override in subclasses or implement a shared offloader that\n",
    "        knows how to iterate keys / export values. Keeping it as a stub here\n",
    "        avoids forcing a storage design too early.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(f\"{self.__class__.__name__}.offload() not implemented.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bd11440",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_HINT = Union[datetime, str]\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class DateRangePacket:\n",
    "    \"\"\"\n",
    "    Simple container for start/end date ranges with both datetime and string formats.\n",
    "    \"\"\"\n",
    "    start_date: DATE_HINT\n",
    "    end_date: DATE_HINT\n",
    "    start_str: Optional[str] = None\n",
    "    end_str: Optional[str] = None\n",
    "    maturity_date: DATE_HINT = None\n",
    "    maturity_str: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        self.start_date = to_datetime(self.start_date)\n",
    "        self.end_date = to_datetime(self.end_date)\n",
    "        if self.maturity_date is not None:\n",
    "            self.maturity_date = to_datetime(self.maturity_date)\n",
    "\n",
    "        self.start_str = self.start_str or self.start_date.strftime(\"%Y-%m-%d\")\n",
    "        self.end_str = self.end_str or self.end_date.strftime(\"%Y-%m-%d\")\n",
    "        if self.maturity_date is not None:\n",
    "            self.maturity_str = self.maturity_str or self.maturity_date.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            self.maturity_str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7e0ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left, bisect_right\n",
    "from datetime import date\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def slice_schedule(full_schedule: List, val_date: date, mat_date: date) -> List:\n",
    "    \"\"\"\n",
    "    Return entries in full_schedule with entry.date in [val_date, mat_date].\n",
    "    Assumes full_schedule is sorted by entry.date ascending and each entry has .date (datetime.date).\n",
    "    \"\"\"\n",
    "    dates = [e.date for e in full_schedule]  # small list; ok to rebuild (or precompute once)\n",
    "    i0 = bisect_left(dates, val_date)\n",
    "    i1 = bisect_right(dates, mat_date)\n",
    "    return full_schedule[i0:i1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ae2ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trade.helpers.helper import ny_now\n",
    "def _should_save_today(max_date: date) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if data should be saved today based on the max_date and current time in New York.\n",
    "    \"\"\"\n",
    "    today = date.today()\n",
    "    current_hour = ny_now().hour\n",
    "    return max_date >= today and current_hour >= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1aba7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_available_on_date(date: date) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the given date is a business day and not a US holiday, False otherwise.\n",
    "    \"\"\"\n",
    "    date = to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    return is_busday(date) and not is_USholiday(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5c7ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _data_structure_cache_it(self: BaseDataManager, \n",
    "                             key: str, \n",
    "                             value: Union[pd.Series, pd.DataFrame],\n",
    "                             *, \n",
    "                             expire: Optional[int] = None):\n",
    "    \"\"\"Merges and caches rate timeseries, excluding today's partial data.\"\"\"\n",
    "    value = value.copy()\n",
    "    if not isinstance(value, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(f\"Expected pd.Series or pd.DataFrame for caching, got {type(value)}\")\n",
    "    \n",
    "    if not isinstance(value.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"Expected DatetimeIndex for caching timeseries data.\")\n",
    "    \n",
    "    if not isinstance(self, BaseDataManager):\n",
    "        raise TypeError(f\"{self.__class__.__name__} must be a subclass of BaseDataManager.\")\n",
    "    \n",
    "    \n",
    "    ## Since it is a timeseries, we will append to existing if exists\n",
    "    existing = self.get(key, default=None)\n",
    "    if existing is not None:\n",
    "        # Merge existing and new values. We're expecting pd.Series\n",
    "        merged = pd.concat([existing, value])\n",
    "        value = merged[~merged.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    if value.empty:\n",
    "        logger.info(f\"Not caching empty timeseries for key: {key}\")\n",
    "        return\n",
    "\n",
    "    if not _should_save_today(max_date=value.index.max().date()):\n",
    "        logger.info(f\"Cutting off today's data for key: {key} to avoid saving partial day data.\")\n",
    "        value = value[value.index < pd.to_datetime(date.today())]\n",
    "\n",
    "    value.sort_index(inplace=True)\n",
    "\n",
    "    self.set(key, value, expire=expire)\n",
    "\n",
    "\n",
    "def _simple_list_cache_it(\n",
    "        self: BaseDataManager,\n",
    "        key: str,\n",
    "        value: List[Any],\n",
    "        *,\n",
    "        expire: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Cache a list of simple values. Will append and keep unique. Also sort\"\"\"\n",
    "\n",
    "    if not isinstance(value, list):\n",
    "        raise TypeError(f\"Expected list. Recieved {type(value)}\")\n",
    "    \n",
    "    existing: List = self.get(key, default = [])\n",
    "    existing.extend(value)\n",
    "    existing = sorted(list(set(existing)))\n",
    "    self.set(key, value, expire=expire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2018f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_structure_sanitize(df: Union[pd.Series, pd.DataFrame],\n",
    "                             start: Union[datetime, str],\n",
    "                             end: Union[datetime, str],) -> Union[pd.Series, pd.DataFrame]:\n",
    "    \"\"\"Sanitizes the data structure by removing duplicates and sorting the index.\"\"\"\n",
    "    print(f\"Sanitizing data from {start} to {end}...\")\n",
    "    if not isinstance(df, (pd.Series, pd.DataFrame)):\n",
    "        raise TypeError(f\"Expected pd.Series or pd.DataFrame for sanitization, got {type(df)}\")\n",
    "    \n",
    "    # Ensure DatetimeIndex. If not, attempt conversion\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        try: \n",
    "            df.index = to_datetime(df.index, format=\"%Y-%m-%d\")\n",
    "        except Exception as e:\n",
    "            raise TypeError(\"Expected DatetimeIndex for sanitization of timeseries data.\") from e\n",
    "        \n",
    "    \n",
    "    # Remove duplicates, keeping the last occurrence\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    \n",
    "    # Sort the index\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # if dataframe, lower case columns\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Filter by start and end dates\n",
    "    df = df[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
    "\n",
    "    # Re-sort after filtering\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Index name=datetime\n",
    "    df.index.name = \"datetime\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9089c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_cache_for_timeseries_data_structure(\n",
    "    self: BaseDataManager,\n",
    "    key: str,\n",
    "    start_dt: DATE_HINT,\n",
    "    end_dt: DATE_HINT,\n",
    ") -> Tuple[Optional[Union[pd.Series, pd.DataFrame]], bool, DATE_HINT, DATE_HINT]:\n",
    "    \"\"\"\n",
    "    Checks cache for existing timeseries data structure and identifies missing dates.\n",
    "\n",
    "    Return args order:\n",
    "    - cached_data: The cached pd.Series or pd.DataFrame if fully present, else None\n",
    "    - is_partial: True if some dates are missing, False if fully present\n",
    "    - missing_start_date: The earliest missing date if partially present, else start_dt\n",
    "    - missing_end_date: The latest missing date if partially present, else end_dt\n",
    "    \"\"\"\n",
    "\n",
    "    cached_data = self.get(key, default=None)\n",
    "    if not isinstance(self, BaseDataManager):\n",
    "        raise TypeError(f\"{self.__class__.__name__} must be a subclass of BaseDataManager.\")\n",
    "    \n",
    "    if not isinstance(cached_data, (pd.Series, pd.DataFrame, type(None))):\n",
    "        return None, False, start_dt, end_dt\n",
    "    \n",
    "    if cached_data is None:\n",
    "        return None, False, start_dt, end_dt\n",
    "    \n",
    "    missing = get_missing_dates(cached_data, _start=start_dt, _end=end_dt)\n",
    "    if not missing:\n",
    "        logger.info(f\"Cache hit for timeseries data structure key: {key}\")\n",
    "        cached_data = _data_structure_sanitize(\n",
    "            cached_data,\n",
    "            start=start_dt,\n",
    "            end=end_dt,\n",
    "        )\n",
    "        return cached_data, False, start_dt, end_dt\n",
    "    logger.info(\n",
    "        f\"Cache partially covers requested date range for timeseries data structure. \"\n",
    "        f\"Key: {key}. Fetching missing dates: {missing}\"\n",
    "    )\n",
    "    return cached_data, True, min(missing), max(missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6e535",
   "metadata": {},
   "source": [
    "### Dividends DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b6bce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionSpotEndpointSource(Enum):\n",
    "    \"\"\"\n",
    "    Thetadata creates a native EOD report every day by 6pm ET.\n",
    "    This enum allows choosing between using that EOD report or the intraday quote end point.\n",
    "    This is essential because during market hours, the EOD report is not yet available.\n",
    "    \"\"\"\n",
    "\n",
    "    EOD = \"eod\"\n",
    "    QUOTE = \"quote\"\n",
    "\n",
    "class OptionPricingModel(Enum):\n",
    "    \"\"\"Enumeration of option pricing model.\"\"\"\n",
    "\n",
    "    BSM = \"Black-Scholes\"\n",
    "    BINOMIAL = \"Binomial\"\n",
    "\n",
    "\n",
    "class VolatilityModel(Enum):\n",
    "    \"\"\"Enumeration of volatility model.\"\"\"\n",
    "\n",
    "    MARKET = \"market\"\n",
    "    MODEL_DYNAMIC = \"model_dynamic\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptionDataConfig(metaclass=SingletonMetaClass):\n",
    "    \"\"\"Configuration for OptionDataManager.\"\"\"\n",
    "\n",
    "    option_spot_endpoint_source: OptionSpotEndpointSource = OptionSpotEndpointSource.EOD\n",
    "    default_lookback_years: int = DIVIDEND_LOOKBACK_YEARS\n",
    "    default_forecast_method: DiscreteDivGrowthModel = DiscreteDivGrowthModel.CONSTANT\n",
    "    dividend_type: DivType = DivType.DISCRETE\n",
    "    include_special_dividends: bool = False\n",
    "    option_model: OptionPricingModel = OptionPricingModel.BSM\n",
    "    volatility_model: VolatilityModel = VolatilityModel.MARKET\n",
    "\n",
    "    def assert_valid(self) -> None:\n",
    "        \"\"\"Validates all configuration values against business rules.\"\"\"\n",
    "        assert self.default_lookback_years > 0, \"Lookback years must be positive.\"\n",
    "        assert self.default_lookback_years <= 5, \"Lookback years seems too large. Max 5.\"\n",
    "        assert isinstance(\n",
    "            self.default_forecast_method, DiscreteDivGrowthModel\n",
    "        ), \"Invalid forecast method. Expected DiscreteDivGrowthModel Enum.\"\n",
    "        assert isinstance(self.dividend_type, DivType), \"Invalid dividend type. Expected DivType Enum.\"\n",
    "        assert isinstance(self.include_special_dividends, bool), \"include_special_dividends must be a boolean.\"\n",
    "        assert isinstance(self.option_spot_endpoint_source, OptionSpotEndpointSource), (\n",
    "            \"Invalid option_spot_endpoint_source. Expected OptionSpotEndpointSource Enum.\"\n",
    "        )\n",
    "        assert isinstance(self.option_model, OptionPricingModel), \"Invalid option_model. Expected OptionPricingModel Enum.\"\n",
    "        assert isinstance(self.volatility_model, VolatilityModel), \"Invalid volatility_model. Expected VolatilityModel Enum.\"\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Validates configuration after initialization.\"\"\"\n",
    "        self.assert_valid()\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        \"\"\"Validates configuration after any attribute change.\"\"\"\n",
    "        super().__setattr__(name, value)\n",
    "        self.assert_valid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83047047",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How dividends timeseries will work:\n",
    "## If discrete:\n",
    "    ## All constant(+...)  will cache up to < today\n",
    "    ## All None Constant will not cache\n",
    "## If continuous:\n",
    "    ## Rely on MarktetTimeseries to provide continuous dividend yield history. It already caches.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01570149",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Result:\n",
    "    \"\"\"Base class for all data manager result containers.\"\"\"\n",
    "\n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides additional fields for string representation. Override in subclasses.\"\"\"\n",
    "        return {}\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Returns string representation with additional fields from subclass.\"\"\"\n",
    "        additional_fields = self._additional_repr_fields()\n",
    "        if additional_fields:\n",
    "            fields_str = \", \".join(f\"{k}={v!r}\" for k, v in additional_fields.items())\n",
    "            return f\"{self.__class__.__name__}({fields_str})\"\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "@dataclass\n",
    "class DividendsResult(Result):\n",
    "    \"\"\"Contains dividend schedule or yield data for a date range.\"\"\"\n",
    "    daily_discrete_dividends: Optional[pd.Series] = None\n",
    "    daily_continuous_dividends: Optional[pd.Series] = None\n",
    "    dividend_type: Optional[DivType] = None\n",
    "    key: Optional[str] = None\n",
    "    undo_adjust: Optional[bool] = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return super().__repr__()\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Checks if dividend data is missing or empty.\"\"\"\n",
    "        if self.dividend_type == DivType.DISCRETE:\n",
    "            return self.daily_discrete_dividends is None or self.daily_discrete_dividends.empty\n",
    "        elif self.dividend_type == DivType.CONTINUOUS:\n",
    "            return self.daily_continuous_dividends is None or self.daily_continuous_dividends.empty\n",
    "        return True\n",
    "    \n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides dividend-specific fields for string representation.\"\"\"\n",
    "        return {\n",
    "            \"dividend_type\": self.dividend_type,\n",
    "            \"key\": self.key,\n",
    "            \"is_empty\": self.is_empty(),\n",
    "            \"undo_adjust\": self.undo_adjust,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fee6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DividendDataManager(BaseDataManager):\n",
    "    \"\"\"Manages dividend data retrieval, caching, and schedule construction for a specific symbol.\"\"\"\n",
    "    CACHE_NAME: ClassVar[str] = \"dividend_data_manager\"\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"] = SeriesId.HIST\n",
    "    CONFIG = OptionDataConfig()\n",
    "    INSTANCES = {}\n",
    "\n",
    "    def __new__(cls, symbol: str, *args: Any, **kwargs: Any) -> \"DividendDataManager\":\n",
    "        \"\"\"Returns cached instance for symbol, creating new one if needed.\"\"\"\n",
    "        if symbol not in cls.INSTANCES:\n",
    "            TS.load_timeseries(symbol, start_date=OPTION_TIMESERIES_START_DATE, end_date=datetime.now())\n",
    "            instance = super(DividendDataManager, cls).__new__(cls)\n",
    "            cls.INSTANCES[symbol] = instance\n",
    "        return cls.INSTANCES[symbol]\n",
    "    \n",
    "    def __init__(self, symbol: str, *, cache_spec: Optional[CacheSpec] = None, enable_namespacing: bool = False) -> None:\n",
    "        \"\"\"Initializes manager for a symbol with cache and temp cache for short-lived data.\"\"\"\n",
    "\n",
    "        if getattr(self, \"_initialized\", False):\n",
    "            return\n",
    "        self._initialized = True\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "        self.symbol = symbol\n",
    "        self.temp_cache: CustomCache = CustomCache(location=DM_GEN_PATH.as_posix(), \n",
    "                                                   fname=\"dividend_temp_cache\", \n",
    "                                                   expire_days=1, \n",
    "                                                   clear_on_exit=True)\n",
    "\n",
    "    ## General caching logic\n",
    "    def cache_it(self, \n",
    "                 key: str, \n",
    "                 value: Any, \n",
    "                 *, \n",
    "                 expire: Optional[int] = None, \n",
    "                 _type: str = \"discrete\") -> None:\n",
    "        \"\"\"Caches dividend data with merge logic for discrete dividends (no future dates).\"\"\"\n",
    "\n",
    "        \n",
    "        ## If discrete dividends, we first check if key exists\n",
    "        ## If it does, we add to it. Only values <= today.\n",
    "        ## If it does not, we create new entry\n",
    "        if _type == \"discrete\":\n",
    "            existing = self.get(key, default=None)\n",
    "            today = datetime.today().date()\n",
    "            allowed = [e for e in value if e.date <= today]\n",
    "\n",
    "            if existing is not None:\n",
    "                # Merge existing and new values. We're expecting lists of ScheduleEntry\n",
    "                merged = existing + allowed\n",
    "                \n",
    "                ## Unique by date\n",
    "                merged = {entry.date: entry for entry in merged}\n",
    "                uniques = sorted(merged.values(), key=lambda e: e.date)\n",
    "                self.set(key, uniques, expire=expire)\n",
    "                return\n",
    "            else:\n",
    "                self.set(key, allowed, expire=expire)\n",
    "                return\n",
    "\n",
    "        # For other types or if no existing, just setattr\n",
    "        self.set(key, value, expire=expire)\n",
    "\n",
    "    ## Dividend yield history retrieval for continuous dividends. Already cached in MarketTimeseries.\n",
    "    def get_div_yield_history(self, symbol: str, skip_preload_check: bool = False) -> pd.Series:\n",
    "        \"\"\"Retrieves continuous dividend yield history from MarketTimeseries.\"\"\"\n",
    "        div_history = TS.get_timeseries(symbol, skip_preload_check=skip_preload_check)\n",
    "        return div_history.dividend_yield\n",
    "\n",
    "    ## Discrete dividend schedule retrieval with caching.\n",
    "    def get_discrete_dividend_schedule(\n",
    "        self,\n",
    "        *,\n",
    "        end_date: Union[str, datetime, pd.Timestamp],\n",
    "        start_date: Union[str, datetime, pd.Timestamp],\n",
    "        valuation_date: Optional[Union[str, datetime, pd.Timestamp]] = None,\n",
    "    ) -> Tuple[List[ScheduleEntry], str]:\n",
    "        \"\"\"Returns discrete dividend schedule between dates with partial cache support.\"\"\"\n",
    "        \n",
    "        start_str = datetime.strftime(start_date, \"%Y-%m-%d\") if isinstance(start_date, (datetime, pd.Timestamp)) else start_date\n",
    "        end_str = datetime.strftime(end_date, \"%Y-%m-%d\") if isinstance(end_date, (datetime, pd.Timestamp)) else end_date\n",
    "        ticker = self.symbol\n",
    "        method = self.CONFIG.default_forecast_method.value\n",
    "        lookback_years = self.CONFIG.default_lookback_years\n",
    "        key = self.make_key(\n",
    "            symbol=ticker,\n",
    "            artifact_type=ArtifactType.DIVS,\n",
    "            series_id=SeriesId.HIST,\n",
    "            method=method,\n",
    "            lookback_years=lookback_years,\n",
    "            current_state=\"schedule\",\n",
    "            interval=Interval.NA,\n",
    "            vendor=\"yfinance\"\n",
    "        )\n",
    "\n",
    "        available_schedule = self.get(key, default=None)\n",
    "        if available_schedule:\n",
    "            logger.info(f\"Cache hit for key: {key}\")\n",
    "            ## If max date in available schedule >= end_date, we can use cache\n",
    "            max_cached_date = max(entry.date for entry in available_schedule)\n",
    "            min_cached_date = min(entry.date for entry in available_schedule)\n",
    "            fully_covered = (min_cached_date <= datetime.strptime(start_str, \"%Y-%m-%d\").date()) and (\n",
    "                max_cached_date >= datetime.strptime(end_str, \"%Y-%m-%d\").date()\n",
    "            )\n",
    "            if fully_covered:\n",
    "                logger.info(f\"Cache fully covers requested date range. Key: {key}\")\n",
    "\n",
    "                ## Filter to requested date range\n",
    "                start_dt = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "                end_dt = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "                filtered_schedule = [e for e in available_schedule if start_dt <= e.date <= end_dt]\n",
    "                return filtered_schedule, key\n",
    "            else:\n",
    "                logger.info(f\"Cache partially covers requested date range. Key: {key}. Fetching missing data.\")\n",
    "\n",
    "        schedule = get_vectorized_dividend_scehdule(\n",
    "            tickers=[ticker],\n",
    "            end_dates=[end_date],\n",
    "            start_dates=[start_date],\n",
    "            method=method,\n",
    "            lookback_years=lookback_years,\n",
    "            valuation_dates=[valuation_date] if valuation_date else None,\n",
    "        )\n",
    "        raw_schedule = schedule[0].schedule\n",
    "        self.cache_it(key, raw_schedule, _type=\"discrete\")\n",
    "\n",
    "        return raw_schedule, key\n",
    "\n",
    "    ## Switcher to choose between constructing all the way or using cached pieces\n",
    "    def _get_discrete_schedule_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        maturity_date: Union[datetime, str],\n",
    "        div_type: Optional[DivType] = None,\n",
    "        undo_adjust: bool = True,\n",
    "    ) -> Tuple[pd.Series, str]:\n",
    "        \"\"\"Builds daily dividend schedule series with partial cache merging and split adjustment.\"\"\"\n",
    "        logger.info(f\"Fetching discrete dividend schedule timeseries for {self.symbol} from {start_date} to {end_date} with maturity {maturity_date}\")\n",
    "        div_type = DivType(div_type) if div_type is not None else self.CONFIG.dividend_type\n",
    "        is_partial = False\n",
    "        start_dt = pd.to_datetime(start_date).date()\n",
    "        end_dt = pd.to_datetime(end_date).date()\n",
    "        mat_dt = pd.to_datetime(maturity_date).date()\n",
    "        start_str = datetime.strftime(start_dt, \"%Y-%m-%d\")\n",
    "        end_str = datetime.strftime(end_dt, \"%Y-%m-%d\")\n",
    "        mat_str = datetime.strftime(mat_dt, \"%Y-%m-%d\")\n",
    "        \n",
    "        if mat_dt < start_dt:\n",
    "            print(f\"Maturity date {mat_dt} is before start date {start_dt}\")\n",
    "            raise ValueError(\"maturity_date must be >= start_date\")\n",
    "\n",
    "        key = self.make_key(\n",
    "            symbol=self.symbol,\n",
    "            artifact_type=ArtifactType.DIVS,\n",
    "            series_id=SeriesId.HIST,\n",
    "            method=self.CONFIG.default_forecast_method.value,\n",
    "            lookback_years=self.CONFIG.default_lookback_years,\n",
    "            current_state=\"schedule_timeseries\",\n",
    "            interval=Interval.EOD,\n",
    "            undo_adjust=undo_adjust,\n",
    "            maturity=mat_str,\n",
    "        )\n",
    "\n",
    "        cached_series = self.get(key, default=None)\n",
    "        if cached_series is not None:\n",
    "            logger.info(f\"Cache hit for discrete schedule timeseries key: {key}\")\n",
    "            missing_dates = get_missing_dates(\n",
    "                cached_series,\n",
    "                start_str,\n",
    "                end_str\n",
    "            )\n",
    "            if not missing_dates:\n",
    "                logger.info(f\"Cache fully covers requested date range for timeseries. Key: {key}\")\n",
    "                cached_series = cached_series[\n",
    "                    (cached_series.index >= pd.to_datetime(start_date)) \n",
    "                    & (cached_series.index <= pd.to_datetime(end_date))\n",
    "            ]\n",
    "                return cached_series, key\n",
    "            else:\n",
    "                logger.info(f\"Cache partially covers requested date range for timeseries. Key: {key}. Fetching missing dates: {missing_dates}\")\n",
    "                start_str, end_str = min(missing_dates), max(missing_dates)\n",
    "                is_partial = True\n",
    "        else:\n",
    "            logger.info(f\"No cache found for discrete schedule timeseries key: {key}. Building from scratch.\")\n",
    "        \n",
    "        # Build from scratch for missing dates\n",
    "        # Fetch ONCE: all events from start_date to maturity_date\n",
    "        full_schedule, _ = self.get_discrete_dividend_schedule(\n",
    "            start_date=start_str,\n",
    "            end_date=mat_str,\n",
    "            valuation_date=start_str,\n",
    "        )\n",
    "\n",
    "        # Build daily schedules efficiently using a moving pointer\n",
    "        series = {}\n",
    "        date_range = pd.date_range(start=start_dt, end=end_dt, freq=\"B\").strftime(\"%Y-%m-%d\")\n",
    "        for d in date_range:\n",
    "            if d in HOLIDAY_SET:\n",
    "                # Skip holidays\n",
    "                continue\n",
    "            d_date = datetime.strptime(d, \"%Y-%m-%d\").date()\n",
    "\n",
    "            ## Simple filter approach\n",
    "            series[d_date] = Schedule(slice_schedule(full_schedule, d_date, mat_dt))\n",
    "        data = pd.Series(series, name=\"dividend_schedule\")\n",
    "        \n",
    "        # Back-adjust to represent cashflows as of valuation date. Ie undoing splits\n",
    "        if undo_adjust:\n",
    "            data = data.to_frame()\n",
    "            split_factors = TS._split_factor[self.symbol].copy()\n",
    "            data[\"split_factor\"] = split_factors\n",
    "            data[\"dividend_schedule\"] = data[\"dividend_schedule\"] * data[\"split_factor\"]\n",
    "            data = data[\"dividend_schedule\"]\n",
    "        \n",
    "        # Cache the constructed timeseries\n",
    "        if is_partial:\n",
    "            # Merge with existing cached series\n",
    "            merged = pd.concat([cached_series, data])\n",
    "            data = merged[~merged.index.duplicated(keep='last')]\n",
    "        \n",
    "        data = _data_structure_sanitize(data, start_date, end_date)\n",
    "        \n",
    "        self.set(key, data, expire=86400/2)  # 12 hours expiry for timeseries cache\n",
    "        return data, key\n",
    "\n",
    "    def get_schedule_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        maturity_date: Union[datetime, str],\n",
    "        div_type: Optional[DivType] = None,\n",
    "        undo_adjust: bool = True,\n",
    "    ) -> DividendsResult:\n",
    "        \"\"\"\n",
    "        Returns a DAILY series (indexed by date) where each value is the dividend schedule\n",
    "        from that valuation date up to maturity_date.\n",
    "\n",
    "        - start_date/end_date define the valuation date range\n",
    "        - maturity_date is the fixed horizon (e.g., option expiry)\n",
    "        \"\"\"\n",
    "\n",
    "        div_type = DivType(div_type) if div_type is not None else self.CONFIG.dividend_type\n",
    "        result = DividendsResult()\n",
    "        result.dividend_type = div_type\n",
    "        result.undo_adjust = undo_adjust\n",
    "\n",
    "        if div_type == DivType.DISCRETE:\n",
    "            data, key = self._get_discrete_schedule_timeseries(\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                maturity_date=maturity_date,\n",
    "                div_type=div_type,\n",
    "                undo_adjust=undo_adjust,\n",
    "            )\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            data.index.name = \"datetime\"\n",
    "            data = data[\n",
    "                (data.index >= pd.to_datetime(start_date)) \n",
    "                & (data.index <= pd.to_datetime(end_date))]\n",
    "            data = data.sort_index()\n",
    "            data = data.drop_duplicates()\n",
    "            result.daily_discrete_dividends = data\n",
    "            result.key = key\n",
    "\n",
    "        elif div_type == DivType.CONTINUOUS:\n",
    "            start_str = pd.to_datetime(start_date).strftime(\"%Y-%m-%d\") if isinstance(start_date, datetime) else start_date\n",
    "            end_str = pd.to_datetime(end_date).strftime(\"%Y-%m-%d\") if isinstance(end_date, datetime) else end_date\n",
    "            yield_history = self.get_div_yield_history(self.symbol, skip_preload_check=True)\n",
    "            filtered = yield_history[(yield_history.index >= start_str) & (yield_history.index <= end_str)]\n",
    "            result.daily_continuous_dividends = filtered\n",
    "            result.key = None\n",
    "        return result\n",
    "        \n",
    "    ## RT Enabled\n",
    "    def get_schedule(\n",
    "        self,\n",
    "        valuation_date: Union[datetime, str],\n",
    "        maturity_date: Union[datetime, str],\n",
    "        div_type: Optional[DivType] = None,\n",
    "        undo_adjust: bool = True,\n",
    "    ) -> DividendsResult:\n",
    "        \"\"\"Returns dividend schedule for a single valuation date to maturity.\"\"\"\n",
    "        \n",
    "        \n",
    "        div_type = DivType(div_type) if div_type is not None else self.CONFIG.dividend_type\n",
    "\n",
    "        val_str = valuation_date.strftime(\"%Y-%m-%d\") if isinstance(valuation_date, datetime) else valuation_date\n",
    "        mat_str = maturity_date.strftime(\"%Y-%m-%d\") if isinstance(maturity_date, datetime) else maturity_date\n",
    "\n",
    "        if div_type == DivType.DISCRETE:\n",
    "            data, key = self.get_discrete_dividend_schedule(\n",
    "                start_date=val_str,\n",
    "                end_date=mat_str,\n",
    "                valuation_date=val_str,  # optional, but consistent\n",
    "            )\n",
    "            if undo_adjust:\n",
    "                split_factor = TS._split_factor[self.symbol].loc[pd.to_datetime(val_str)]\n",
    "            else:\n",
    "                split_factor = 1.0\n",
    "            data = Schedule(schedule=[entry * split_factor for entry in data])\n",
    "            data = pd.Series({val_str: data})\n",
    "        elif div_type == DivType.CONTINUOUS:\n",
    "            data = self.get_div_yield_history(self.symbol)\n",
    "            data = data[(data.index >= pd.to_datetime(valuation_date)) & (data.index <= pd.to_datetime(maturity_date))]\n",
    "            key = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dividend type: {div_type}\")\n",
    "\n",
    "        result = DividendsResult()\n",
    "        \n",
    "        if div_type == DivType.DISCRETE:\n",
    "            result.daily_discrete_dividends = data\n",
    "        else:\n",
    "            result.daily_continuous_dividends = data\n",
    "        result.key = key\n",
    "        result.undo_adjust = undo_adjust\n",
    "        result.dividend_type = div_type\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def offload(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Example implementation of offload for DividendDataManager.\n",
    "        \"\"\"\n",
    "        print(f\"No offload logic implemented for {self.CACHE_NAME}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85c8b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:34 [test] trade.__init__ INFO: Signal function for `_on_exit` added to signal number 15.\n",
      "2026-01-18 19:43:34 [test] trade.__init__ INFO: Signal function for `_on_exit` added to signal number 2.\n",
      "2026-01-18 19:43:34 [test] trade.__init__ INFO: Exit handler `_on_exit` registered for normal program exit.\n",
      "2026-01-18 19:43:34 [test] __main__ INFO: Cache hit for key: symbol:AAPL|interval:na|artifact_type:divs|series_id:hist|current_state:SCHEDULE|lookback_years:1|method:CONSTANT|vendor:YFINANCE\n",
      "2026-01-18 19:43:34 [test] __main__ INFO: Cache partially covers requested date range. Key: symbol:AAPL|interval:na|artifact_type:divs|series_id:hist|current_state:SCHEDULE|lookback_years:1|method:CONSTANT|vendor:YFINANCE. Fetching missing data.\n",
      "2026-01-18 19:43:34 [test] trade.optionlib.assets.dividend INFO: Using dual projection method for ticker AAPL\n",
      "2026-01-18 19:43:35 [test] trade.optionlib.assets.dividend INFO: Expected Dividend Size before adjustment: 17, for original valuation: 9. Size from historical divs: 16\n",
      "2026-01-18 19:43:35 [test] trade.optionlib.assets.dividend INFO: Expected Dividend Size to be projected: 1\n",
      "2026-01-18 19:43:35 [test] trade.optionlib.assets.dividend INFO: Projected Dividend List: [0.26]\n",
      "2026-01-18 19:43:35 [test] trade.optionlib.assets.dividend INFO: Combined Dividend List: [0.22, 0.23, 0.23, 0.23, 0.23, 0.24, 0.24, 0.24, 0.24, 0.25, 0.25, 0.25, 0.25, 0.26, 0.26, 0.26, 0.26]\n",
      "2026-01-18 19:43:35 [test] trade.optionlib.assets.dividend INFO: Combined Date List: [datetime.date(2022, 2, 4), datetime.date(2022, 5, 6), datetime.date(2022, 8, 5), datetime.date(2022, 11, 4), datetime.date(2023, 2, 10), datetime.date(2023, 5, 12), datetime.date(2023, 8, 11), datetime.date(2023, 11, 10), datetime.date(2024, 2, 9), datetime.date(2024, 5, 10), datetime.date(2024, 8, 12), datetime.date(2024, 11, 8), datetime.date(2025, 2, 10), datetime.date(2025, 5, 12), datetime.date(2025, 8, 11), datetime.date(2025, 11, 10), datetime.date(2026, 2, 10)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<ScheduleEntry: 2024-02-09 - 0.24>,\n",
       "  <ScheduleEntry: 2024-05-10 - 0.25>,\n",
       "  <ScheduleEntry: 2024-08-12 - 0.25>,\n",
       "  <ScheduleEntry: 2024-11-08 - 0.25>,\n",
       "  <ScheduleEntry: 2025-02-10 - 0.25>,\n",
       "  <ScheduleEntry: 2025-05-12 - 0.26>,\n",
       "  <ScheduleEntry: 2025-08-11 - 0.26>,\n",
       "  <ScheduleEntry: 2025-11-10 - 0.26>],\n",
       " 'symbol:AAPL|interval:na|artifact_type:divs|series_id:hist|current_state:SCHEDULE|lookback_years:1|method:CONSTANT|vendor:YFINANCE')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdiv = DividendDataManager(symbol=\"AAPL\")\n",
    "testdiv.get_discrete_dividend_schedule(\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2025-12-31\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1eef60d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:35 [test] __main__ INFO: Fetching discrete dividend schedule timeseries for AAPL from 2025-01-01 to 2026-01-14 with maturity 2026-10-29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:35 [test] __main__ INFO: Cache hit for discrete schedule timeseries key: symbol:AAPL|interval:eod|artifact_type:divs|series_id:hist|current_state:SCHEDULE_TIMESERIES|lookback_years:1|maturity:2026-10-29|method:CONSTANT|undo_adjust:1\n",
      "2026-01-18 19:43:35 [test] __main__ INFO: Cache fully covers requested date range for timeseries. Key: symbol:AAPL|interval:eod|artifact_type:divs|series_id:hist|current_state:SCHEDULE_TIMESERIES|lookback_years:1|maturity:2026-10-29|method:CONSTANT|undo_adjust:1\n",
      "Discrete Dividends Schedule Timeseries:\n",
      "datetime\n",
      "2025-01-02    ((2025-02-10, 0.25), (2025-05-12, 0.26), (2025...\n",
      "2025-01-03    ((2025-02-10, 0.25), (2025-05-12, 0.26), (2025...\n",
      "2025-01-06    ((2025-02-10, 0.25), (2025-05-12, 0.26), (2025...\n",
      "2025-01-07    ((2025-02-10, 0.25), (2025-05-12, 0.26), (2025...\n",
      "2025-01-08    ((2025-02-10, 0.25), (2025-05-12, 0.26), (2025...\n",
      "                                    ...                        \n",
      "2026-01-08    ((2026-02-10, 0.26), (2026-05-10, 0.26), (2026...\n",
      "2026-01-09    ((2026-02-10, 0.26), (2026-05-10, 0.26), (2026...\n",
      "2026-01-12    ((2026-02-10, 0.26), (2026-05-10, 0.26), (2026...\n",
      "2026-01-13    ((2026-02-10, 0.26), (2026-05-10, 0.26), (2026...\n",
      "2026-01-14    ((2026-02-10, 0.26), (2026-05-10, 0.26), (2026...\n",
      "Name: dividend_schedule, Length: 259, dtype: object\n"
     ]
    }
   ],
   "source": [
    "testdiv.CONFIG.dividend_type = DivType.DISCRETE\n",
    "d = testdiv.get_schedule_timeseries(\n",
    "    start_date=\"2025-01-01\",\n",
    "    end_date=\"2026-01-14\",\n",
    "    maturity_date=\"2026-10-29\",\n",
    "    undo_adjust=True,\n",
    ")\n",
    "\n",
    "# d2 = testdiv.get_schedule(\n",
    "#     valuation_date=\"2026-01-15\",\n",
    "#     maturity_date=\"2026-05-31\",\n",
    "#     undo_adjust=False,\n",
    "# )\n",
    "print(\"Discrete Dividends Schedule Timeseries:\")\n",
    "print(d.daily_discrete_dividends)\n",
    "# print(\"Discrete Dividends Schedule at specific valuation date:\")\n",
    "# print(d2.daily_discrete_dividends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "547d12bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'NVDA', 'TSLA', 'COST', 'AMZN', 'META', 'AMD', 'SBUX', 'NFLX', 'BA']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS._spot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36691b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DividendsResult(dividend_type=<DivType.DISCRETE: 'discrete'>, key='symbol:AAPL|interval:eod|artifact_type:divs|series_id:hist|current_state:SCHEDULE_TIMESERIES|lookback_years:1|maturity:2026-10-29|method:CONSTANT|undo_adjust:1', is_empty=False, undo_adjust=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8137ae34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-10-10    0.001665\n",
       "2022-10-11    0.001683\n",
       "2022-10-12    0.001690\n",
       "2022-10-13    0.001635\n",
       "2022-10-14    0.001690\n",
       "                ...   \n",
       "2025-10-27    0.000968\n",
       "2025-10-28    0.000967\n",
       "2025-10-29    0.000965\n",
       "2025-10-30    0.000959\n",
       "2025-10-31    0.000963\n",
       "Length: 800, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdiv.CONFIG.dividend_type = DivType.CONTINUOUS\n",
    "d = testdiv.get_schedule_timeseries(\n",
    "    start_date=\"2022-10-08\",\n",
    "    end_date=\"2025-10-31\",\n",
    "    maturity_date=\"2025-10-31\",\n",
    ")\n",
    "d.daily_continuous_dividends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b4078",
   "metadata": {},
   "source": [
    "## Rates Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06b02699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "def deannualize(annual_rate, periods=365):\n",
    "    \"\"\"Converts annual rate to per-period rate.\"\"\"\n",
    "    return (1 + annual_rate) ** (1 / periods) - 1\n",
    "\n",
    "@dataclass\n",
    "class RatesResult(Result):\n",
    "    \"\"\"Contains risk-free rate data for a date range.\"\"\"\n",
    "    daily_risk_free_rates: Optional[pd.Series] = None\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Checks if rate data is missing or empty.\"\"\"\n",
    "        return self.daily_risk_free_rates is None or self.daily_risk_free_rates.empty\n",
    "    \n",
    "    def _additional_repr_fields(self):\n",
    "        \"\"\"Provides rate-specific fields for string representation.\"\"\"\n",
    "        return {\n",
    "            \"is_empty\": self.is_empty(),\n",
    "        }\n",
    "    def __repr__(self) -> str:\n",
    "        return super().__repr__()\n",
    "    \n",
    "\n",
    "class RatesDataManager(BaseDataManager):\n",
    "    \"\"\"Singleton manager for risk-free rate data from treasury bills (^IRX).\"\"\"\n",
    "    CACHE_NAME: ClassVar[str] = \"rates_data_manager\"\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"] = SeriesId.HIST\n",
    "    INSTANCE = None\n",
    "    DEFAULT_YFINANCE_TICKER = \"^IRX\"  # 13 WEEK TREASURY BILL\n",
    "    CONFIG: OptionDataConfig = OptionDataConfig()\n",
    "    \n",
    "    def __new__(\n",
    "        cls,\n",
    "        *,\n",
    "        cache_spec: Optional[CacheSpec] = None,\n",
    "        enable_namespacing: bool = False,\n",
    "    ) -> \"RatesDataManager\":\n",
    "        \"\"\"Ensures only one instance exists (singleton pattern).\"\"\"\n",
    "        \n",
    "        if cls.INSTANCE is not None:\n",
    "            return cls.INSTANCE\n",
    "        instance = super(RatesDataManager, cls).__new__(cls)\n",
    "        cls.INSTANCE = instance\n",
    "        return instance\n",
    "    \n",
    "    def __init__(self, *, cache_spec: Optional[CacheSpec] = None, enable_namespacing: bool = False) -> None:\n",
    "        \"\"\"Initializes singleton instance once, skipping subsequent calls.\"\"\"\n",
    "        if getattr(self, \"_init_called\", False):\n",
    "            return\n",
    "        self._init_called = True\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "\n",
    "    def get_rate(\n",
    "        self,\n",
    "        date: Union[datetime, str],\n",
    "        interval: Interval = Interval.EOD,\n",
    "        str_interval: Optional[str] = None,\n",
    "    ) -> RatesResult:\n",
    "        \"\"\"Returns risk-free rate for a single date.\"\"\"\n",
    "        \n",
    "        if not is_available_on_date(to_datetime(date).date()):\n",
    "            logger.warning(f\"Requested date {date} is not a business day or is a US holiday. Returning empty RatesResult.\")\n",
    "            return RatesResult(daily_risk_free_rates=pd.Series(dtype=float))\n",
    "        date_str = pd.to_datetime(date).strftime(\"%Y-%m-%d\") if isinstance(date, datetime) else date\n",
    "        \n",
    "        rates_data = self.get_risk_free_rate_timeseries(\n",
    "            start_date=date_str,\n",
    "            end_date=date_str,\n",
    "            interval=interval,\n",
    "            str_interval=str_interval,\n",
    "        )\n",
    "        rate = rates_data.daily_risk_free_rates\n",
    "        if rate is not None and not rate.empty:\n",
    "            rate = rate.iloc[0:1]\n",
    "            \n",
    "\n",
    "        return RatesResult(daily_risk_free_rates=rate)\n",
    "\n",
    "    def get_risk_free_rate_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        interval: Interval = Interval.EOD,\n",
    "        str_interval: Optional[str] = None,\n",
    "    ) -> RatesResult:\n",
    "        \"\"\"Returns risk-free rate timeseries with partial cache support.\"\"\"\n",
    "        \n",
    "        start_str = pd.to_datetime(start_date).strftime(\"%Y-%m-%d\") if isinstance(start_date, datetime) else start_date\n",
    "        end_str = pd.to_datetime(end_date).strftime(\"%Y-%m-%d\") if isinstance(end_date, datetime) else end_date\n",
    "        \n",
    "        ## Make cache key\n",
    "        key = self.make_key(\n",
    "            symbol=self.DEFAULT_YFINANCE_TICKER,\n",
    "            artifact_type=ArtifactType.RATES,\n",
    "            series_id=SeriesId.HIST,\n",
    "            interval=interval,\n",
    "        )\n",
    "        \n",
    "        ## Determine yfinance interval\n",
    "        if not str_interval:\n",
    "            fn_interval = \"1d\" if interval == Interval.EOD else \"30m\"\n",
    "        else:\n",
    "            fn_interval = str_interval\n",
    "        \n",
    "        ## Check cache\n",
    "        series = self.get(key, default=None)\n",
    "\n",
    "        ## Check if cache covers requested date range\n",
    "        if series is not None:\n",
    "            logger.info(f\"Cache hit for risk-free rate timeseries key: {key}\")\n",
    "            missing = get_missing_dates(\n",
    "                series,\n",
    "                pd.to_datetime(start_date).strftime(\"%Y-%m-%d\"),\n",
    "                pd.to_datetime(end_date).strftime(\"%Y-%m-%d\"),\n",
    "            )\n",
    "\n",
    "\n",
    "            ## If no missing dates, return cached series\n",
    "            if not missing:\n",
    "                logger.info(f\"Cache fully covers requested date range for risk-free rate timeseries. Key: {key}\")\n",
    "                series = _data_structure_sanitize(\n",
    "                    series,\n",
    "                    start=start_str,\n",
    "                    end=end_str,\n",
    "                )\n",
    "                return RatesResult(daily_risk_free_rates=series)\n",
    "            else:\n",
    "                ## Fetch missing dates\n",
    "                start_date = min(missing)\n",
    "                end_date = max(missing)\n",
    "                logger.info(f\"Cache partially covers requested date range for risk-free rate timeseries. Key: {key}. Fetching missing dates: {missing}\")\n",
    "        else:\n",
    "            logger.info(f\"No cache found for risk-free rate timeseries key: {key}. Fetching from source.\")\n",
    "\n",
    "\n",
    "        # Fetch rates data\n",
    "        rates_data = self._query_yfinance(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            interval=fn_interval,\n",
    "        )[\"annualized\"]\n",
    "\n",
    "        if series is not None:\n",
    "            # Merge with existing cached series\n",
    "            merged = pd.concat([series, rates_data])\n",
    "            rates_data = merged[~merged.index.duplicated(keep='last')]\n",
    "        \n",
    "        ## Cache the updated series\n",
    "        self.cache_it(key, rates_data)\n",
    "        \n",
    "        ## Sanitize before returning\n",
    "        rates_data = _data_structure_sanitize(\n",
    "            rates_data,\n",
    "            start=start_str, # Ensure only requested range\n",
    "            end=end_str,\n",
    "        )\n",
    "\n",
    "        return RatesResult(rates_data)\n",
    "\n",
    "    def cache_it(self, key, value, *, expire=None):\n",
    "        \"\"\"Merges and caches rate timeseries, excluding today's partial data.\"\"\"\n",
    "        ## Since it is a timeseries, we will append to existing if exists\n",
    "        _data_structure_cache_it(self, key, value, expire=expire)\n",
    "\n",
    "        \n",
    "    def _query_yfinance(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        interval: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Fetches ^IRX treasury bill rates from yfinance and formats output.\"\"\"\n",
    "\n",
    "        ## Date buffer to ensure we get all data\n",
    "        start_date = to_datetime(start_date) - pd.Timedelta(days=5)\n",
    "        end_date = to_datetime(end_date) + pd.Timedelta(days=5)\n",
    "    \n",
    "        data_min = yf.download(\n",
    "            \"^IRX\",\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            interval=interval,\n",
    "            progress=False,\n",
    "            multi_level_index=False,\n",
    "        )\n",
    "\n",
    "        data_min.columns = data_min.columns.str.lower()\n",
    "        data_min[\"daily\"] = data_min[\"close\"].apply(deannualize)\n",
    "        data_min[\"annualized\"] = data_min[\"close\"] / 100\n",
    "        data_min[\"name\"] = \"^IRX\"\n",
    "        data_min[\"description\"] = \"13 WEEK TREASURY BILL\"\n",
    "        data_min.index.name = \"Datetime\"\n",
    "        data_min = data_min[[\"name\", \"description\", \"daily\", \"annualized\"]]\n",
    "        data_min = data_min[(data_min.index >= pd.to_datetime(start_date)) & (data_min.index <= pd.to_datetime(end_date))]\n",
    "        return data_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e8192d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:36 [test] __main__ INFO: Cache hit for risk-free rate timeseries key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "2026-01-18 19:43:36 [test] __main__ INFO: Cache fully covers requested date range for risk-free rate timeseries. Key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "Sanitizing data from 2026-01-09 to 2026-01-12...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-09    0.03513\n",
       "2026-01-12    0.03533\n",
       "Name: annualized, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_manager = RatesDataManager()\n",
    "\n",
    "rates_result = rt_manager.get_risk_free_rate_timeseries(\n",
    "    start_date=\"2026-01-09\",\n",
    "    end_date=\"2026-01-12\",\n",
    ")\n",
    "rates_result.daily_risk_free_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03d1e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:36 [test] __main__ INFO: Cache hit for risk-free rate timeseries key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "2026-01-18 19:43:36 [test] __main__ INFO: Cache fully covers requested date range for risk-free rate timeseries. Key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "Sanitizing data from 2026-01-13 to 2026-01-13...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-13    0.0356\n",
       "Name: annualized, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_manager.get_rate(\n",
    "    date=pd.Timestamp(\"2026-01-13\"),\n",
    ").daily_risk_free_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3aaea",
   "metadata": {},
   "source": [
    "## Forward Price (Mostly for black scholes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33758a10",
   "metadata": {},
   "source": [
    "### Forward DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8646955",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ForwardResult(Result):\n",
    "    \"\"\"Contains forward price data (discrete or continuous dividend model).\"\"\"\n",
    "    daily_discrete_forward: Optional[pd.Series] = None\n",
    "    daily_continuous_forward: Optional[pd.Series] = None\n",
    "    dividend_type: Optional[DivType] = None\n",
    "    key: Optional[str] = None\n",
    "    dividend_result: Optional[DividendsResult] = None\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Checks if forward price data is missing or empty.\"\"\"\n",
    "        if self.dividend_type == DivType.DISCRETE:\n",
    "            return self.daily_discrete_forward is None or self.daily_discrete_forward.empty\n",
    "        elif self.dividend_type == DivType.CONTINUOUS:\n",
    "            return self.daily_continuous_forward is None or self.daily_continuous_forward.empty\n",
    "        return True\n",
    "    \n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides forward-specific fields for string representation.\"\"\"\n",
    "        return {\n",
    "            \"dividend_type\": self.dividend_type,\n",
    "            \"key\": self.key,\n",
    "            \"is_empty\": self.is_empty(),\n",
    "        }\n",
    "    def __repr__(self) -> str:\n",
    "        return super().__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1a1f0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDataManager(BaseDataManager):\n",
    "    \"\"\"Manages forward price computation and caching for a specific symbol using spot, rates, and dividends.\"\"\"\n",
    "    CACHE_NAME: ClassVar[str] = \"forward_data_manager\"\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"] = SeriesId.HIST\n",
    "    INSTANCES = {}\n",
    "    # CONFIG = ForwardsConfig()\n",
    "\n",
    "    def __new__(cls, symbol: str, *args: Any, **kwargs: Any) -> \"ForwardDataManager\":\n",
    "        \"\"\"Returns cached instance for symbol, creating new one if needed.\"\"\"\n",
    "        if symbol not in cls.INSTANCES:\n",
    "            TS.load_timeseries(symbol, start_date=OPTION_TIMESERIES_START_DATE, end_date=datetime.now())\n",
    "            instance = super(ForwardDataManager, cls).__new__(cls)\n",
    "            cls.INSTANCES[symbol] = instance\n",
    "        return cls.INSTANCES[symbol]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        symbol: str,\n",
    "        *,\n",
    "        cache_spec: Optional[CacheSpec] = None,\n",
    "        enable_namespacing: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes manager once per symbol instance.\"\"\"\n",
    "        if getattr(self, \"_initialized\", False):\n",
    "            return\n",
    "\n",
    "        self._initialized = True\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def _normalize_inputs(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        maturity_date: Union[datetime, str],\n",
    "        div_type: Optional[DivType],\n",
    "    ) -> Tuple[DivType, date, date, date, str, str, str]:\n",
    "        \"\"\"Converts date inputs to both date objects and strings.\"\"\"\n",
    "        div_type = DivType(div_type) if div_type is not None else DivType.DISCRETE\n",
    "\n",
    "        start_dt = (\n",
    "            datetime.strptime(start_date, \"%Y-%m-%d\") if isinstance(start_date, str) else start_date\n",
    "        )\n",
    "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if isinstance(end_date, str) else end_date\n",
    "        mat_dt = (\n",
    "            datetime.strptime(maturity_date, \"%Y-%m-%d\")\n",
    "            if isinstance(maturity_date, str)\n",
    "            else maturity_date\n",
    "        )\n",
    "\n",
    "        start_str = datetime.strftime(start_dt, \"%Y-%m-%d\")\n",
    "        end_str = datetime.strftime(end_dt, \"%Y-%m-%d\")\n",
    "        mat_str = datetime.strftime(mat_dt, \"%Y-%m-%d\")\n",
    "        return div_type, start_dt, end_dt, mat_dt, start_str, end_str, mat_str\n",
    "\n",
    "    def _build_key(self, *, mat_str: str, div_type: DivType, use_chain_spot: bool) -> str:\n",
    "        \"\"\"Constructs cache key from maturity, dividend type, and spot type.\"\"\"\n",
    "        return self.make_key(\n",
    "            symbol=self.symbol,\n",
    "            artifact_type=ArtifactType.FWD,\n",
    "            series_id=SeriesId.HIST,\n",
    "            maturity=mat_str,\n",
    "            div_type=div_type.value,\n",
    "            use_chain_spot=use_chain_spot,\n",
    "            interval=Interval.EOD,\n",
    "        )\n",
    "\n",
    "    def _try_get_cached(\n",
    "        self,\n",
    "        *,\n",
    "        key: str,\n",
    "        start_str: str,\n",
    "        end_str: str,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        div_type: DivType,\n",
    "    ) -> Tuple[Optional[pd.Series], bool, str, str, Optional[ForwardResult]]:\n",
    "        \"\"\"Checks cache for existing data and identifies missing dates.\"\"\"\n",
    "        cached_series = self.get(key, default=None)\n",
    "        if cached_series is None:\n",
    "            return None, False, start_str, end_str, None\n",
    "\n",
    "        missing = get_missing_dates(cached_series, _start=start_str, _end=end_str)\n",
    "        if not missing:\n",
    "            logger.info(f\"Cache hit for forward timeseries key: {key}\")\n",
    "            cached_series = _data_structure_sanitize(\n",
    "                cached_series,\n",
    "                start=start_str,\n",
    "                end=end_str,\n",
    "            )\n",
    "\n",
    "            result = ForwardResult()\n",
    "            if div_type == DivType.DISCRETE:\n",
    "                result.daily_discrete_forward = cached_series\n",
    "            else:\n",
    "                result.daily_continuous_forward = cached_series\n",
    "            result.dividend_type = div_type\n",
    "            result.key = key\n",
    "            return cached_series, False, start_str, end_str, result\n",
    "\n",
    "        logger.info(\n",
    "            f\"Cache partially covers requested date range for forward timeseries. \"\n",
    "            f\"Key: {key}. Fetching missing dates: {missing}\"\n",
    "        )\n",
    "        return cached_series, True, min(missing), max(missing), None\n",
    "\n",
    "    def _get_dividend_result(\n",
    "        self,\n",
    "        *,\n",
    "        start_str: str,\n",
    "        end_str: str,\n",
    "        mat_str: str,\n",
    "        div_type: DivType,\n",
    "        dividend_result: Optional[DividendsResult],\n",
    "        use_chain_spot: bool,\n",
    "    ) -> DividendsResult:\n",
    "        \"\"\"Fetches or validates dividend data with adjustment consistency checks.\"\"\"\n",
    "        if dividend_result is None:\n",
    "            dividend_result = DividendDataManager(symbol=self.symbol).get_schedule_timeseries(\n",
    "                start_date=start_str,\n",
    "                end_date=end_str,\n",
    "                maturity_date=mat_str,\n",
    "                div_type=div_type,\n",
    "                undo_adjust=use_chain_spot,  # If using chain spot, back adjust dividends\n",
    "            )\n",
    "\n",
    "        if dividend_result.is_empty():\n",
    "            raise ValueError(\"Dividend result is empty. Cannot compute forward prices without dividend information.\")\n",
    "\n",
    "        if dividend_result.undo_adjust != use_chain_spot:\n",
    "            raise ValueError(\"Mismatch between dividend_result.undo_adjust and use_chain_spot. They must be the same.\")\n",
    "\n",
    "        return dividend_result\n",
    "\n",
    "    def _load_spot(self, *, use_chain_spot: bool, spot: Optional[TimeseriesData] = None) -> pd.Series:\n",
    "        \"\"\"Loads spot or chain_spot price series.\"\"\"\n",
    "        if spot is None:\n",
    "            spot = TS.get_timeseries(self.symbol, skip_preload_check=True)\n",
    "        if use_chain_spot:\n",
    "            return spot.chain_spot[\"close\"]\n",
    "        return spot.spot[\"close\"]\n",
    "\n",
    "    def _load_rates(self, *, start_str: str, end_str: str, rates: Optional[RatesResult] = None) -> pd.Series:\n",
    "        \"\"\"Loads risk-free rates for date range.\"\"\"\n",
    "        if rates is None:\n",
    "            rates_data = RatesDataManager().get_risk_free_rate_timeseries(\n",
    "                start_date=start_str,\n",
    "                end_date=end_str,\n",
    "                interval=Interval.EOD,\n",
    "            )\n",
    "            rates = rates_data.daily_risk_free_rates\n",
    "        else:\n",
    "            rates = rates.daily_risk_free_rates\n",
    "        rates = rates[(rates.index >= pd.to_datetime(start_str)) & (rates.index <= pd.to_datetime(end_str))]\n",
    "        return rates\n",
    "\n",
    "    def _align_3(\n",
    "        self, spot: pd.Series, rates: pd.Series, third: pd.Series, *, third_name: str\n",
    "    ) -> Tuple[pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"Aligns three series to common dates and validates no NaNs.\"\"\"\n",
    "        idx = spot.index.intersection(rates.index).intersection(third.index)\n",
    "\n",
    "        spot = spot.reindex(idx)\n",
    "        rates = rates.reindex(idx)\n",
    "        third = third.reindex(idx)\n",
    "\n",
    "        if rates.isna().any():\n",
    "            raise ValueError(\"NaNs in rates after alignment.\")\n",
    "        if third.isna().any():\n",
    "            raise ValueError(f\"NaNs in {third_name} after alignment.\")\n",
    "\n",
    "        return spot, rates, third\n",
    "\n",
    "    def _compute_forward_discrete(\n",
    "        self,\n",
    "        *,\n",
    "        spot: pd.Series,\n",
    "        rates: pd.Series,\n",
    "        discrete_divs: pd.Series,  # series of Schedule objects\n",
    "        mat_dt: date,\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"Computes forward prices using discrete dividend schedules.\"\"\"\n",
    "\n",
    "        \n",
    "        pv_divs = vectorized_discrete_pv(\n",
    "            schedules=discrete_divs.to_list(),\n",
    "            r=rates.tolist(),\n",
    "            _valuation_dates=discrete_divs.index.tolist(),\n",
    "            _end_dates=[mat_dt] * len(discrete_divs),\n",
    "        )\n",
    "        pv_divs = [pv_divs] if isinstance(pv_divs, (int, float)) else pv_divs \n",
    "\n",
    "        second_vector = [(mat_dt - val).days * SECONDS_IN_DAY for val in discrete_divs.index]\n",
    "        t = [val / SECONDS_IN_YEAR for val in second_vector]\n",
    "\n",
    "\n",
    "        forwards = vectorized_forward_discrete(\n",
    "            S=spot.tolist(),\n",
    "            r=rates.tolist(),\n",
    "            T=t,\n",
    "            pv_divs=pv_divs,\n",
    "        )\n",
    "        return pd.Series(data=forwards, index=discrete_divs.index)\n",
    "\n",
    "    def _compute_forward_continuous(\n",
    "        self,\n",
    "        *,\n",
    "        spot: pd.Series,\n",
    "        rates: pd.Series,\n",
    "        continuous_divs: pd.Series,  # series of dividend yields\n",
    "        mat_dt: date,\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"Computes forward prices using continuous dividend yields.\"\"\"\n",
    "        q_factor = get_vectorized_continuous_dividends(\n",
    "            div_rates=continuous_divs.tolist(),\n",
    "            _valuation_dates=continuous_divs.index.tolist(),\n",
    "            _end_dates=[mat_dt] * len(continuous_divs),\n",
    "        )\n",
    "\n",
    "        second_vector = [(mat_dt - val).days * SECONDS_IN_DAY for val in continuous_divs.index]\n",
    "        t = [val / SECONDS_IN_YEAR for val in second_vector]\n",
    "\n",
    "        forwards = vectorized_forward_continuous(\n",
    "            S=spot.tolist(),\n",
    "            r=rates.tolist(),\n",
    "            T=t,\n",
    "            q_factor=q_factor,\n",
    "        )\n",
    "        return pd.Series(data=forwards, index=continuous_divs.index)\n",
    "\n",
    "    def _merge_partial(self, cached_series: pd.Series, forward_series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Merges newly computed data with cached data, keeping latest values.\"\"\"\n",
    "        merged = pd.concat([cached_series, forward_series])\n",
    "        forward_series = merged[~merged.index.duplicated(keep=\"last\")]\n",
    "        return forward_series\n",
    "\n",
    "\n",
    "\n",
    "    def get_forward_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        maturity_date: Union[datetime, str],\n",
    "        div_type: Optional[DivType] = None,\n",
    "        spot: Optional[TimeseriesData] = None,\n",
    "        rates: Optional[RatesResult] = None,\n",
    "        *,\n",
    "        dividend_result: Optional[DividendsResult] = None,\n",
    "        use_chain_spot: bool = True,\n",
    "    ) -> ForwardResult:\n",
    "        \"\"\"\n",
    "        Returns a DAILY series (indexed by date) where each value is the forward price\n",
    "        from that valuation date up to maturity_date.\n",
    "\n",
    "        - start_date/end_date define the valuation date range\n",
    "        - maturity_date is the fixed horizon (e.g., option expiry)\n",
    "        \"\"\"\n",
    "        result = ForwardResult()\n",
    "        og_start_date = start_date\n",
    "        og_end_date = end_date\n",
    "        div_type, start_dt, end_dt, mat_dt, start_str, end_str, mat_str = self._normalize_inputs(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            maturity_date=maturity_date,\n",
    "            div_type=div_type,\n",
    "        )\n",
    "\n",
    "        if mat_dt < start_dt:\n",
    "            raise ValueError(\"maturity_date must be >= start_date\")\n",
    "\n",
    "        key = self._build_key(mat_str=mat_str, div_type=div_type, use_chain_spot=use_chain_spot)\n",
    "\n",
    "        cached_series, partial_hit, start_str, end_str, cached_result = self._try_get_cached(\n",
    "            key=key,\n",
    "            start_str=start_str,\n",
    "            end_str=end_str,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            div_type=div_type,\n",
    "        )\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "        \n",
    "        dividend_result = self._get_dividend_result(\n",
    "            start_str=start_str,\n",
    "            end_str=end_str,\n",
    "            mat_str=mat_str,\n",
    "            div_type=div_type,\n",
    "            dividend_result=dividend_result,\n",
    "            use_chain_spot=use_chain_spot,\n",
    "        )\n",
    "\n",
    "        spot = self._load_spot(use_chain_spot=use_chain_spot, spot=spot) \n",
    "        rates = self._load_rates(start_str=start_str, end_str=end_str, rates=rates)\n",
    "\n",
    "        if div_type == DivType.DISCRETE:\n",
    "            discrete_divs = dividend_result.daily_discrete_dividends\n",
    "\n",
    "            spot, rates, discrete_divs = self._align_3(\n",
    "                spot=spot,\n",
    "                rates=rates,\n",
    "                third=discrete_divs,\n",
    "                third_name=\"discrete dividend schedules\",\n",
    "            )\n",
    "\n",
    "            forward_series = self._compute_forward_discrete(\n",
    "                spot=spot,\n",
    "                rates=rates,\n",
    "                discrete_divs=discrete_divs,\n",
    "                mat_dt=mat_dt,\n",
    "            )\n",
    "\n",
    "            result.daily_discrete_forward = forward_series\n",
    "            result.dividend_result = dividend_result\n",
    "\n",
    "        elif div_type == DivType.CONTINUOUS:\n",
    "            continuous_divs = dividend_result.daily_continuous_dividends\n",
    "\n",
    "            spot, rates, continuous_divs = self._align_3(\n",
    "                spot=spot,\n",
    "                rates=rates,\n",
    "                third=continuous_divs,\n",
    "                third_name=\"div yields\",\n",
    "            )\n",
    "\n",
    "            forward_series = self._compute_forward_continuous(\n",
    "                spot=spot,\n",
    "                rates=rates,\n",
    "                continuous_divs=continuous_divs,\n",
    "                mat_dt=mat_dt,\n",
    "            )\n",
    "\n",
    "            result.daily_continuous_forward = forward_series\n",
    "            result.dividend_result = dividend_result\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dividend type: {div_type}\")\n",
    "\n",
    "        result.dividend_type = div_type\n",
    "        result.key = key\n",
    "\n",
    "        if partial_hit:\n",
    "            forward_series = self._merge_partial(cached_series=cached_series, forward_series=forward_series)\n",
    "\n",
    "        self.cache_it(key, forward_series, expire=86400 / 2)  # 12 hours expiry\n",
    "\n",
    "        forward_series = _data_structure_sanitize(\n",
    "            forward_series,\n",
    "            start=og_start_date,\n",
    "            end=og_end_date,\n",
    "        )\n",
    "\n",
    "        if div_type == DivType.DISCRETE:\n",
    "            result.daily_discrete_forward = forward_series\n",
    "        else:\n",
    "            result.daily_continuous_forward = forward_series\n",
    "\n",
    "        return result\n",
    "\n",
    "    def make_key(self, *, symbol, interval=None, artifact_type=None, series_id=None, **extra_parts):\n",
    "        \"\"\"Delegates to BaseDataManager key construction.\"\"\"\n",
    "        return super().make_key(\n",
    "            symbol=symbol, interval=interval, artifact_type=artifact_type, series_id=series_id, **extra_parts\n",
    "        )\n",
    "\n",
    "    def cache_it(self, key, value, *, expire=None):\n",
    "        \"\"\"Merges and caches forward timeseries, excluding today's partial data.\"\"\"\n",
    "        ## Since it is a timeseries, we will append to existing if exists\n",
    "        _data_structure_cache_it(self, key, value, expire=expire)\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_forward(self, \n",
    "                    date: Union[datetime, str], \n",
    "                    maturity_date: Union[datetime, str],\n",
    "                    div_type: Optional[DivType] = None,\n",
    "                    dividend_result: Optional[DividendsResult] = None,\n",
    "                    spot: Optional[TimeseriesData] = None,\n",
    "                    rates: Optional[RatesResult] = None,\n",
    "                    *, \n",
    "                    use_chain_spot: bool = True) -> ForwardResult:\n",
    "        \"\"\"\n",
    "        Returns the forward price at a specific valuation datetime\n",
    "        div_type = DivType(div_type) if div_type is not None else DivType.DISCRETE\n",
    "        \"\"\"\n",
    "        div_type = DivType(div_type) if div_type is not None else DivType.DISCRETE\n",
    "        date_str = date.strftime(\"%Y-%m-%d\") if isinstance(date, datetime) else date\n",
    "        mat_str = maturity_date.strftime(\"%Y-%m-%d\") if isinstance(maturity_date, datetime) else maturity_date\n",
    "        start = date_str\n",
    "        end = date_str\n",
    "\n",
    "        result = self.get_forward_timeseries(\n",
    "            start_date=start,\n",
    "            end_date=end,\n",
    "            maturity_date=mat_str,\n",
    "            div_type=div_type,\n",
    "            use_chain_spot=use_chain_spot,\n",
    "            dividend_result=dividend_result,\n",
    "            spot=spot,\n",
    "            rates=rates,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "        \n",
    "\n",
    "    def offload(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Example implementation of offload for ForwardDataManager.\n",
    "        \"\"\"\n",
    "        print(f\"No offload logic implemented for {self.CACHE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6810ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_test = ForwardDataManager(symbol=\"COST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08eb286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:37 [test] __main__ INFO: Cache hit for risk-free rate timeseries key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "2026-01-18 19:43:37 [test] __main__ INFO: Cache fully covers requested date range for risk-free rate timeseries. Key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "Sanitizing data from 2026-01-10 to 2026-01-14...\n",
      "2026-01-18 19:43:37 [test] __main__ INFO: Cache hit for forward timeseries key: symbol:COST|interval:eod|artifact_type:forward|series_id:hist|div_type:DISCRETE|maturity:2026-01-20|use_chain_spot:0\n",
      "Sanitizing data from 2026-01-10 to 2026-01-14...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-12    943.810580\n",
       "2026-01-13    942.573305\n",
       "2026-01-14    951.536662\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fwd_discrete = fwd_test.get_forward_timeseries(\n",
    "    start_date=\"2026-01-10\",\n",
    "    end_date=\"2026-01-14\",\n",
    "    maturity_date=\"2026-01-20\",\n",
    "    div_type=DivType.DISCRETE,\n",
    "    use_chain_spot=False,\n",
    "    spot=TS.get_timeseries(\"COST\", skip_preload_check=True),\n",
    "    rates=RatesDataManager().get_risk_free_rate_timeseries(\n",
    "        start_date=\"2026-01-10\",\n",
    "        end_date=\"2026-01-14\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# fwd_cont = fwd_test.get_forward_timeseries(\n",
    "#     start_date=\"2025-01-02\",\n",
    "#     end_date=\"2026-01-15\",\n",
    "#     maturity_date=\"2026-01-02\",\n",
    "#     div_type=DivType.CONTINUOUS,\n",
    "#     use_chain_spot=False,\n",
    "# )\n",
    "fwd_discrete.daily_discrete_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ce9318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:38 [test] __main__ INFO: Cache hit for key: symbol:AAPL|interval:na|artifact_type:divs|series_id:hist|current_state:SCHEDULE|lookback_years:1|method:CONSTANT|vendor:YFINANCE\n",
      "2026-01-18 19:43:38 [test] __main__ INFO: Cache partially covers requested date range. Key: symbol:AAPL|interval:na|artifact_type:divs|series_id:hist|current_state:SCHEDULE|lookback_years:1|method:CONSTANT|vendor:YFINANCE. Fetching missing data.\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Using dual projection method for ticker AAPL\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Expected Dividend Size before adjustment: 12, for original valuation: 4. Size from historical divs: 8\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Expected Dividend Size to be projected: 4\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Projected Dividend List: [0.26, 0.26, 0.26, 0.26]\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Combined Dividend List: [0.24, 0.25, 0.25, 0.25, 0.25, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26]\n",
      "2026-01-18 19:43:38 [test] trade.optionlib.assets.dividend INFO: Combined Date List: [datetime.date(2024, 2, 9), datetime.date(2024, 5, 10), datetime.date(2024, 8, 12), datetime.date(2024, 11, 8), datetime.date(2025, 2, 10), datetime.date(2025, 5, 12), datetime.date(2025, 8, 11), datetime.date(2025, 11, 10), datetime.date(2026, 2, 10), datetime.date(2026, 5, 10), datetime.date(2026, 8, 10), datetime.date(2026, 11, 10)]\n",
      "2026-01-18 19:43:38 [test] __main__ INFO: Cache hit for risk-free rate timeseries key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "2026-01-18 19:43:38 [test] __main__ INFO: Cache fully covers requested date range for risk-free rate timeseries. Key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "Sanitizing data from 2026-01-14 to 2026-01-14...\n",
      "2026-01-18 19:43:38 [test] __main__ INFO: Cache hit for forward timeseries key: symbol:COST|interval:eod|artifact_type:forward|series_id:hist|div_type:DISCRETE|maturity:2026-01-20|use_chain_spot:0\n",
      "Sanitizing data from 2026-01-14 to 2026-01-14...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-14    951.536662\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div = testdiv.get_schedule(\n",
    "    valuation_date=\"2026-01-14\",\n",
    "    maturity_date=\"2027-01-02\",\n",
    "    div_type=DivType.DISCRETE,\n",
    "    undo_adjust=False,\n",
    ")\n",
    "\n",
    "rate = rt_manager.get_rate(\n",
    "    date=\"2026-01-14\",\n",
    ")\n",
    "\n",
    "spot = TS.get_timeseries(\"AAPL\", skip_preload_check=True, start_date=\"2026-01-14\", end_date=\"2026-01-14\")\n",
    "\n",
    "fwd_test.get_forward(\n",
    "    date=\"2026-01-14\",\n",
    "    maturity_date=\"2026-01-20\",\n",
    "    div_type=DivType.DISCRETE,\n",
    "    use_chain_spot=False,\n",
    "    dividend_result=div,\n",
    "    spot=spot,\n",
    "    rates=rate\n",
    ").daily_discrete_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a58879",
   "metadata": {},
   "source": [
    "## Equity Market Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20d7e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpotResult(Result):\n",
    "    \"\"\"Contains spot price data with optional split adjustment information.\"\"\"\n",
    "    daily_spot: Optional[pd.Series] = None\n",
    "    undo_adjust: Optional[bool] = None\n",
    "    key: Optional[str] = None\n",
    "\n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides spot-specific fields for string representation.\"\"\"\n",
    "        return {\n",
    "            \"key\": self.key,\n",
    "            \"is_empty\": self.daily_spot is None or self.daily_spot.empty,\n",
    "            \"undo_adjust\": self.undo_adjust,\n",
    "        }\n",
    "    def __repr__(self) -> str:\n",
    "        return super().__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b38a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpotDataManager(BaseDataManager):\n",
    "    \"\"\"Manages spot price retrieval for a specific symbol with split adjustment support.\"\"\"\n",
    "    CACHE_NAME: ClassVar[str] = \"spot_data_manager\"\n",
    "    DEFAULT_SERIES_ID: ClassVar[\"SeriesId\"] = SeriesId.HIST\n",
    "    INSTANCES = {}\n",
    "    def __new__(cls, symbol: str, *args: Any, **kwargs: Any) -> \"SpotDataManager\":\n",
    "        \"\"\"Returns cached instance for symbol, creating new one if needed.\"\"\"\n",
    "        if symbol not in cls.INSTANCES:\n",
    "            TS.load_timeseries(symbol, start_date=OPTION_TIMESERIES_START_DATE, end_date=datetime.now())\n",
    "            instance = super(SpotDataManager, cls).__new__(cls)\n",
    "            cls.INSTANCES[symbol] = instance\n",
    "        return cls.INSTANCES[symbol]\n",
    "    \n",
    "    def __init__(self, symbol: str, *, cache_spec: Optional[CacheSpec] = None, enable_namespacing: bool = False) -> None:\n",
    "        \"\"\"Initializes manager once per symbol instance.\"\"\"\n",
    "        if getattr(self, \"_initialized\", False):\n",
    "            return\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def get_spot_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        undo_adjust: bool = True,\n",
    "    ) -> SpotResult:\n",
    "        \"\"\"Returns spot or chain_spot price series for date range from MarketTimeseries.\"\"\"\n",
    "        \n",
    "        timeseries = TS.get_timeseries(self.symbol, skip_preload_check=True, start_date=start_date, end_date=end_date)\n",
    "        if undo_adjust:\n",
    "            spot_series = timeseries.chain_spot[\"close\"]\n",
    "        else:\n",
    "            spot_series = timeseries.spot[\"close\"]\n",
    "        \n",
    "        spot_series = _data_structure_sanitize(\n",
    "            spot_series,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "        )\n",
    "        result = SpotResult()\n",
    "        key = None # No caching key for now\n",
    "        result.daily_spot = spot_series\n",
    "        result.undo_adjust = undo_adjust\n",
    "        result.key = key\n",
    "        return result\n",
    "\n",
    "    def get_at_time(\n",
    "            self,\n",
    "            date: Union[datetime, str],\n",
    "    ) -> AtIndexResult:\n",
    "        \"\"\"Returns spot data at a specific datetime from MarketTimeseries.\"\"\"\n",
    "        \n",
    "        return TS.get_at_index(sym=self.symbol, index=date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa4e6e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitizing data from 2026-01-10 to 2026-01-14...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-12    260.250000\n",
       "2026-01-13    261.049988\n",
       "2026-01-14    259.959991\n",
       "Name: close, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_spot = SpotDataManager(symbol=\"AAPL\")\n",
    "spot_result = test_spot.get_spot_timeseries(\n",
    "    start_date=\"2026-01-10\",\n",
    "    end_date=\"2026-01-14\",\n",
    "    undo_adjust=True,\n",
    ")\n",
    "spot_result.daily_spot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7a86d",
   "metadata": {},
   "source": [
    "## Option Spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f247fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class OptionSpotResult(Result):\n",
    "    \"\"\"Container for option spot price timeseries data.\"\"\"\n",
    "    daily_option_spot: Optional[pd.DataFrame] = None\n",
    "    key: Optional[str] = None\n",
    "    endpoint_source: Optional[OptionSpotEndpointSource] = None\n",
    "\n",
    "    @property\n",
    "    def close(self) -> pd.Series:\n",
    "        if not self.is_empty():\n",
    "            return self.daily_option_spot[\"close\"]\n",
    "        else:\n",
    "            return pd.Series(name=\"close\", index=pd.DatetimeIndex([]), dtype=float)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def midpoint(self) -> pd.Series:\n",
    "        if not self.is_empty():\n",
    "            return self.daily_option_spot[\"midpoint\"]\n",
    "        else:\n",
    "            return pd.Series(name=\"midpoint\", index=pd.DatetimeIndex([]), dtype=float)\n",
    "    \n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Checks if option spot data is missing or empty.\"\"\"\n",
    "        return self.daily_option_spot is None or self.daily_option_spot.empty\n",
    "    \n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides metadata on data presence.\"\"\"\n",
    "        return {\n",
    "            \"key\": self.key,\n",
    "            \"is_empty\": self.is_empty(),\n",
    "            \"endpoint_source\": self.endpoint_source,\n",
    "        }\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Delegates to base Result repr.\"\"\"\n",
    "        return super().__repr__()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9588a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionSpotDataManager(BaseDataManager):\n",
    "    \"\"\"Manages option spot price retrieval for a specific symbol from Thetadata API.\"\"\"\n",
    "    CACHE_NAME: str = \"option_spot_manager\"\n",
    "    DEFAULT_SERIES_ID: str = SeriesId.HIST\n",
    "    CONFIG = OptionDataConfig()\n",
    "    INSTANCES = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        symbol: str,\n",
    "        *,\n",
    "        cache_spec: Optional[CacheSpec] = None,\n",
    "        enable_namespacing: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes manager for a specific symbol.\"\"\"\n",
    "        super().__init__(cache_spec=cache_spec, enable_namespacing=enable_namespacing)\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def _sync_date(\n",
    "        self,\n",
    "        start_date: DATE_HINT,\n",
    "        end_date: DATE_HINT,\n",
    "        strike: Optional[float] = None,\n",
    "        expiration: Optional[Union[datetime, str]] = None,\n",
    "        right: Optional[str] = None,\n",
    "    ) -> Tuple[DATE_HINT, DATE_HINT]:\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "        dates = list_dates(\n",
    "            symbol=self.symbol,\n",
    "            exp=expiration,\n",
    "            right=right,\n",
    "            strike=strike,\n",
    "        )\n",
    "\n",
    "        dates = to_datetime(dates)\n",
    "        min_date, max_date = min(dates), max(dates)\n",
    "        start_date = max(min_date, start_date)\n",
    "        end_date = min(end_date, max_date)\n",
    "\n",
    "        return start_date, end_date\n",
    "    \n",
    "    def get_option_spot(\n",
    "        self,\n",
    "        date: Union[datetime, str],\n",
    "        *,\n",
    "        strike: Optional[float] = None,\n",
    "        expiration: Optional[Union[datetime, str]] = None,\n",
    "        right: Optional[str] = None,\n",
    "        opttick: Optional[str] = None,\n",
    "        endpoint_source: Optional[OptionSpotEndpointSource] = None,\n",
    "    ) -> OptionSpotResult:\n",
    "        \"\"\"Fetches option spot price for a single date from Thetadata API.\"\"\"\n",
    "        date_str = pd.to_datetime(date).strftime(\"%Y-%m-%d\") if isinstance(date, datetime) else date\n",
    "        result = self.get_option_spot_timeseries(\n",
    "            start_date=date_str,\n",
    "            end_date=date_str,\n",
    "            strike=strike,\n",
    "            expiration=expiration,\n",
    "            right=right,\n",
    "            opttick=opttick,\n",
    "            endpoint_source=endpoint_source,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def get_option_spot_timeseries(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        *,\n",
    "        strike: Optional[float] = None,\n",
    "        expiration: Optional[Union[datetime, str]] = None,\n",
    "        right: Optional[str] = None,\n",
    "        opttick: Optional[str] = None,\n",
    "        endpoint_source: Optional[OptionSpotEndpointSource] = None,\n",
    "    ) -> OptionSpotResult:\n",
    "        \"\"\"Fetches option spot price timeseries from Thetadata API.\"\"\"\n",
    "        if endpoint_source is None:\n",
    "            endpoint_source = self.CONFIG.option_spot_endpoint_source\n",
    "        \n",
    "        strike, right, symbol, expiration = _handle_opttick_param(\n",
    "            strike=strike,\n",
    "            right=right,\n",
    "            symbol=self.symbol,\n",
    "            exp=expiration,\n",
    "            opttick=opttick,\n",
    "        )\n",
    "\n",
    "        date_packet = DateRangePacket(start_date=start_date, end_date=end_date)\n",
    "        start_date, end_date = date_packet.start_date, date_packet.end_date\n",
    "        start_date, end_date = self._sync_date(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            strike=float(strike),\n",
    "            expiration=expiration,\n",
    "            right=right,\n",
    "        )\n",
    "        start_str, end_str = date_packet.start_str, date_packet.end_str\n",
    "        \n",
    "        # Construct cache key\n",
    "        key = self.make_key(\n",
    "            symbol=self.symbol,\n",
    "            artifact_type=ArtifactType.OPTION_SPOT,\n",
    "            series_id=SeriesId.HIST,\n",
    "            endpoint_source=endpoint_source.value,\n",
    "            interval=Interval.EOD,\n",
    "            strike=strike,\n",
    "            right=right,\n",
    "            expiration=expiration,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # Check cache\n",
    "        cached_data, is_partial, start_date, end_date = _check_cache_for_timeseries_data_structure(\n",
    "            key=key,\n",
    "            self=self,\n",
    "            start_dt=start_date,\n",
    "            end_dt=end_date,\n",
    "        )\n",
    "\n",
    "        if cached_data is not None and not is_partial:\n",
    "            logger.info(f\"Cache hit for option spot timeseries key: {key}\")\n",
    "            result = OptionSpotResult()\n",
    "            result.daily_option_spot = cached_data\n",
    "            result.key = key\n",
    "            result.endpoint_source = endpoint_source\n",
    "            return result\n",
    "        elif is_partial:\n",
    "            logger.info(f\"Cache partially covers requested date range for option spot timeseries. Key: {key}. Fetching missing dates.\")\n",
    "        else:\n",
    "            logger.info(f\"No cache found for option spot timeseries key: {key}. Fetching from source.\")\n",
    "        \n",
    "        # Fetch data from Thetadata API (placeholder logic)\n",
    "        fetched_data = self._query_thetadata_api(\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            endpoint_source=endpoint_source,\n",
    "            strike=strike,\n",
    "            expiration=expiration,\n",
    "            right=right,\n",
    "        )\n",
    "        \n",
    "        # Merge with cached data if partial\n",
    "        if cached_data is not None and is_partial:\n",
    "            merged = pd.concat([cached_data, fetched_data])\n",
    "            fetched_data = merged[~merged.index.duplicated(keep='last')]\n",
    "\n",
    "        fetched_data.index = default_timestamp(fetched_data.index)\n",
    "\n",
    "        # Cache the fetched data\n",
    "        _data_structure_cache_it(self, key, fetched_data)  # 24 hours expiry\n",
    "\n",
    "        # Sanitize before returning\n",
    "        fetched_data = _data_structure_sanitize(\n",
    "            fetched_data,\n",
    "            start=start_str,\n",
    "            end=end_str,\n",
    "        )\n",
    "        \n",
    "        result = OptionSpotResult()\n",
    "        result.daily_option_spot = fetched_data\n",
    "        result.key = key\n",
    "        result.endpoint_source = endpoint_source\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _query_thetadata_api(\n",
    "        self,\n",
    "        start_date: Union[datetime, str],\n",
    "        end_date: Union[datetime, str],\n",
    "        endpoint_source: OptionSpotEndpointSource,\n",
    "        strike: Optional[float] = None,\n",
    "        expiration: Optional[Union[datetime, str]] = None,\n",
    "        right: Optional[str] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Placeholder method to simulate fetching option spot data from Thetadata API.\"\"\"\n",
    "        # In a real implementation, this method would make HTTP requests to Thetadata's API.\n",
    "        if endpoint_source == OptionSpotEndpointSource.EOD:\n",
    "            return retrieve_eod_ohlc(\n",
    "                symbol=self.symbol,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                strike=float(strike),\n",
    "                exp=expiration,\n",
    "                right=right,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            logger.info(f\"Fetching option spot data from Thetadata Quote endpoint for {self.symbol} from {start_date} to {end_date}.\")\n",
    "        return quote_to_eod_patch(\n",
    "            symbol=self.symbol,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            strike=float(strike),\n",
    "            exp=expiration,\n",
    "            right=right,\n",
    "            ohlc_format=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93e3759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:39 [test] __main__ INFO: Cache hit for timeseries data structure key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:QUOTE|expiration:2028-03-17|right:C|strike:200\n",
      "Sanitizing data from 2026-01-10 00:00:00 to 2026-01-14 00:00:00...\n",
      "2026-01-18 19:43:39 [test] __main__ INFO: Cache hit for option spot timeseries key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:QUOTE|expiration:2028-03-17|right:C|strike:200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>closebid</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>closeask</th>\n",
       "      <th>midpoint</th>\n",
       "      <th>weighted_midpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026-01-12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>86.80</td>\n",
       "      <td>29</td>\n",
       "      <td>87.60</td>\n",
       "      <td>87.200</td>\n",
       "      <td>87.186667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>87.35</td>\n",
       "      <td>12</td>\n",
       "      <td>88.35</td>\n",
       "      <td>87.850</td>\n",
       "      <td>87.981579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-01-14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>86.55</td>\n",
       "      <td>16</td>\n",
       "      <td>87.70</td>\n",
       "      <td>87.125</td>\n",
       "      <td>87.107576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            open    high  low   close  volume  bid_size  closebid  ask_size  \\\n",
       "datetime                                                                      \n",
       "2026-01-12   0.0  88.225  0.0  87.200     NaN        31     86.80        29   \n",
       "2026-01-13   0.0  88.200  0.0  87.850     NaN         7     87.35        12   \n",
       "2026-01-14   0.0  88.300  0.0  87.125     NaN        17     86.55        16   \n",
       "\n",
       "            closeask  midpoint  weighted_midpoint  \n",
       "datetime                                           \n",
       "2026-01-12     87.60    87.200          87.186667  \n",
       "2026-01-13     88.35    87.850          87.981579  \n",
       "2026-01-14     87.70    87.125          87.107576  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_option_manager = OptionSpotDataManager(symbol=\"AAPL\")\n",
    "data = spot_option_manager.get_option_spot_timeseries(\n",
    "    start_date=\"2026-01-10\",\n",
    "    end_date=\"2026-01-14\",\n",
    "    endpoint_source=OptionSpotEndpointSource.QUOTE,\n",
    "    strike=200,\n",
    "    expiration=\"2028-03-17\",\n",
    "    right=\"C\",\n",
    ")\n",
    "data.daily_option_spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df2ff7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:39 [test] __main__ INFO: Cache hit for timeseries data structure key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:EOD|expiration:2028-03-17|right:C|strike:200\n",
      "Sanitizing data from 2026-01-12 00:00:00 to 2026-01-12 00:00:00...\n",
      "2026-01-18 19:43:39 [test] __main__ INFO: Cache hit for option spot timeseries key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:EOD|expiration:2028-03-17|right:C|strike:200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2026-01-12    87.2\n",
       "Name: midpoint, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spot_option_manager = OptionSpotDataManager(symbol=\"AAPL\")\n",
    "data = spot_option_manager.get_option_spot(\n",
    "    date=\"2026-01-12\",\n",
    "    endpoint_source=OptionSpotEndpointSource.EOD,\n",
    "    strike=200,\n",
    "    expiration=\"2028-03-17\",\n",
    "    right=\"C\",\n",
    ")\n",
    "data.midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e8454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root</th>\n",
       "      <th>expiration</th>\n",
       "      <th>strike</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260116</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260116</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260123</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260123</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260130</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260130</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2965</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260206</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260206</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260213</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260213</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260220</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260220</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260227</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260227</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260320</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260320</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260417</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260417</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260515</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2983</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260515</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2985</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260618</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260618</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260717</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260717</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260821</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260821</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260918</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20260918</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20261120</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20261120</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20261218</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20261218</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20270115</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20270115</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20270617</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20270617</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2972</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20271217</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20271217</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20280121</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20280121</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20280317</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20280317</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20281215</td>\n",
       "      <td>200.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20281215</td>\n",
       "      <td>200.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      root  expiration  strike right\n",
       "2874  AAPL    20260116   200.0     C\n",
       "2875  AAPL    20260116   200.0     P\n",
       "2960  AAPL    20260123   200.0     P\n",
       "2962  AAPL    20260123   200.0     C\n",
       "2880  AAPL    20260130   200.0     P\n",
       "2879  AAPL    20260130   200.0     C\n",
       "2965  AAPL    20260206   200.0     P\n",
       "2966  AAPL    20260206   200.0     C\n",
       "2888  AAPL    20260213   200.0     C\n",
       "2887  AAPL    20260213   200.0     P\n",
       "2969  AAPL    20260220   200.0     P\n",
       "2971  AAPL    20260220   200.0     C\n",
       "2891  AAPL    20260227   200.0     C\n",
       "2892  AAPL    20260227   200.0     P\n",
       "2974  AAPL    20260320   200.0     P\n",
       "2975  AAPL    20260320   200.0     C\n",
       "2978  AAPL    20260417   200.0     C\n",
       "2979  AAPL    20260417   200.0     P\n",
       "2982  AAPL    20260515   200.0     C\n",
       "2983  AAPL    20260515   200.0     P\n",
       "2985  AAPL    20260618   200.0     C\n",
       "2984  AAPL    20260618   200.0     P\n",
       "2910  AAPL    20260717   200.0     C\n",
       "2911  AAPL    20260717   200.0     P\n",
       "2990  AAPL    20260821   200.0     P\n",
       "2991  AAPL    20260821   200.0     C\n",
       "2992  AAPL    20260918   200.0     P\n",
       "2993  AAPL    20260918   200.0     C\n",
       "2917  AAPL    20261120   200.0     P\n",
       "2916  AAPL    20261120   200.0     C\n",
       "2926  AAPL    20261218   200.0     C\n",
       "2924  AAPL    20261218   200.0     P\n",
       "2935  AAPL    20270115   200.0     P\n",
       "2937  AAPL    20270115   200.0     C\n",
       "3012  AAPL    20270617   200.0     P\n",
       "3013  AAPL    20270617   200.0     C\n",
       "2972  AAPL    20271217   200.0     P\n",
       "2973  AAPL    20271217   200.0     C\n",
       "3028  AAPL    20280121   200.0     C\n",
       "3029  AAPL    20280121   200.0     P\n",
       "0     AAPL    20280317   200.0     C\n",
       "1     AAPL    20280317   200.0     P\n",
       "33    AAPL    20281215   200.0     P\n",
       "35    AAPL    20281215   200.0     C"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = list_contracts(symbol=\"AAPL\", start_date=\"2026-01-16\")\n",
    "c.sort_values(by=\"expiration\").query(\"strike == 200\").head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99058897",
   "metadata": {},
   "source": [
    "## Vol Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "39e3701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VolatilityResult(Result):\n",
    "    \"\"\"Contains volatility surface data.\"\"\"\n",
    "    timeseries: Optional[pd.Series] = None\n",
    "    key: Optional[str] = None\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Checks if volatility data is missing or empty.\"\"\"\n",
    "        return self.timeseries is None or self.timeseries.empty\n",
    "    def _additional_repr_fields(self) -> Dict[str, Any]:\n",
    "        \"\"\"Provides volatility-specific fields for string representation.\"\"\"\n",
    "        return {\n",
    "            \"key\": self.key,\n",
    "            \"is_empty\": self.is_empty(),\n",
    "        }\n",
    "    def __repr__(self) -> str:\n",
    "        return super().__repr__()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5968b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distance_helper(start: datetime, end: datetime) -> float:\n",
    "    \"\"\"Calculates time distance in years between two dates.\"\"\"\n",
    "    delta = (to_datetime(end) - to_datetime(start)).days * SECONDS_IN_DAY\n",
    "    return delta / SECONDS_IN_YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e47ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache hit for timeseries data structure key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:EOD|expiration:2026-07-17|right:C|strike:200\n",
      "Sanitizing data from 2025-11-20 00:00:00 to 2026-01-16 00:00:00...\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache hit for option spot timeseries key: symbol:AAPL|interval:eod|artifact_type:option_spot|series_id:hist|endpoint_source:EOD|expiration:2026-07-17|right:C|strike:200\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Fetching discrete dividend schedule timeseries for AAPL from 2025-11-20 00:00:00 to 2026-01-16 00:00:00 with maturity 2026-07-17\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache hit for discrete schedule timeseries key: symbol:AAPL|interval:eod|artifact_type:divs|series_id:hist|current_state:SCHEDULE_TIMESERIES|lookback_years:1|maturity:2026-07-17|method:CONSTANT|undo_adjust:1\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache fully covers requested date range for timeseries. Key: symbol:AAPL|interval:eod|artifact_type:divs|series_id:hist|current_state:SCHEDULE_TIMESERIES|lookback_years:1|maturity:2026-07-17|method:CONSTANT|undo_adjust:1\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache hit for risk-free rate timeseries key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "2026-01-18 19:43:41 [test] __main__ INFO: Cache fully covers requested date range for risk-free rate timeseries. Key: symbol:^IRX|interval:eod|artifact_type:rates|series_id:hist\n",
      "Sanitizing data from 2025-11-20 to 2026-01-16...\n",
      "Sanitizing data from 2025-11-20 00:00:00 to 2026-01-16 00:00:00...\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "ts_start = \"2025-01-01\"\n",
    "ts_end = \"2026-01-18\"\n",
    "expiration = \"2026-07-17\"\n",
    "market_price = (\n",
    "    OptionSpotDataManager(symbol=\"AAPL\")\n",
    "    .get_option_spot_timeseries(\n",
    "        start_date=ts_start,\n",
    "        end_date=ts_end,\n",
    "        strike=200,\n",
    "        expiration=expiration,\n",
    "        right=\"C\",\n",
    "    )\n",
    "    .midpoint\n",
    ")\n",
    "dividends = DividendDataManager(symbol=\"AAPL\").get_schedule_timeseries(\n",
    "    start_date=market_price.index.min(),\n",
    "    end_date=market_price.index.max(),\n",
    "    maturity_date=expiration,\n",
    "    div_type=DivType.DISCRETE,\n",
    ")\n",
    "dividends_res = vector_convert_to_time_frac(\n",
    "    schedules=dividends.daily_discrete_dividends,\n",
    "    valuation_dates=dividends.daily_discrete_dividends.index.tolist(),\n",
    "    end_dates=[to_datetime(expiration)] * len(dividends.daily_discrete_dividends),\n",
    ")\n",
    "\n",
    "sigma = [0.2] * len(dividends_res)\n",
    "r = RatesDataManager().get_risk_free_rate_timeseries(\n",
    "    start_date=market_price.index.min(),\n",
    "    end_date=market_price.index.max(),\n",
    ").daily_risk_free_rates\n",
    "T = [time_distance_helper(start=dt, end=expiration) for dt in dividends.daily_discrete_dividends.index]\n",
    "S0 = SpotDataManager(symbol=\"AAPL\").get_spot_timeseries(\n",
    "    start_date=market_price.index.min(),\n",
    "    end_date=market_price.index.max(),\n",
    ").daily_spot\n",
    "\n",
    "right = [\"c\"] * len(dividends.daily_discrete_dividends)\n",
    "dividend_type = [DivType.DISCRETE.value] * len(dividends.daily_discrete_dividends)\n",
    "K = [200.0] * len(dividends.daily_discrete_dividends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b503aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.34927314808915"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 20\n",
    "date = \"2026-01-06\"\n",
    "crr_binomial_pricing(\n",
    "    K = K[i],\n",
    "    T = T[i],\n",
    "    sigma = sigma[i],\n",
    "    r = r.loc[dividends.daily_discrete_dividends.index[i]],\n",
    "    S0 = S0.loc[dividends.daily_discrete_dividends.index[i]],\n",
    "    dividend_type = dividend_type,\n",
    "    dividends = (dividends_res[i].schedule),\n",
    "    option_type = right[i],\n",
    "    N = 100,\n",
    "    american = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afa79aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.35225717942950563)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_crr_implied_volatility(\n",
    "    S=S0.loc[dividends.daily_discrete_dividends.index[i]],\n",
    "    K=K[i],\n",
    "    T=T[i],\n",
    "    r=r.loc[dividends.daily_discrete_dividends.index[i]],\n",
    "    market_price=market_price.loc[dividends.daily_discrete_dividends.index[i]],\n",
    "    dividend_type=dividend_type[i],\n",
    "    q=dividends_res[i].schedule,\n",
    "    option_type=right[i],\n",
    "    N=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "17bd1367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2025-11-20    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-11-21    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-11-24    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-11-25    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-11-26    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-11-28    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-01    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-02    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-03    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-04    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-05    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-08    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-09    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-10    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-11    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-12    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-15    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-16    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-17    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-18    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-19    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-22    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-23    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-24    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-26    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-29    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-30    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2025-12-31    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-02    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-05    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-06    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-07    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-08    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-09    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-12    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-13    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-14    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-15    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "2026-01-16    ((2026-02-10, 0.26), (2026-05-10, 0.26))\n",
       "Name: dividend_schedule, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trade.helpers.helper import get_parrallel_apply, runProcesses, runThreads\n",
    "s = slice(0, 272)\n",
    "s\n",
    "len(r)\n",
    "r\n",
    "S0\n",
    "dividends.daily_discrete_dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "44ae8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71.02630033668731,\n",
       " 76.09229303220053,\n",
       " 80.37353532681382,\n",
       " 81.38808716012909,\n",
       " 81.96444947401316,\n",
       " 83.13808721068547,\n",
       " 87.27711472403652,\n",
       " 90.29754247077118,\n",
       " 88.20660139858977,\n",
       " 84.73446831932573,\n",
       " 82.80205029879087,\n",
       " 81.87362207160093,\n",
       " 81.16549041598502,\n",
       " 82.68146918792709,\n",
       " 81.88873890121882,\n",
       " 82.06495491180878,\n",
       " 77.88984162045772,\n",
       " 78.37087626259574,\n",
       " 75.61148315977978,\n",
       " 75.91054322876582,\n",
       " 77.35073526416885,\n",
       " 74.62624448060924,\n",
       " 75.99617143789405,\n",
       " 77.41758579513007,\n",
       " 76.9557065376728,\n",
       " 77.24149177132259,\n",
       " 76.54931516941679,\n",
       " 75.3282815804333,\n",
       " 74.4296114685626,\n",
       " 70.64548361128149,\n",
       " 65.81246752064467,\n",
       " 63.79651442825223,\n",
       " 62.50227051241733,\n",
       " 62.80793623535227,\n",
       " 63.61869929257374,\n",
       " 64.40448392746192,\n",
       " 63.31367744975112,\n",
       " 61.581577922293064,\n",
       " 58.94114722939313]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_crr_binomial_pricing(\n",
    "    K[s],  # K\n",
    "    T[s],  # T\n",
    "    sigma[s],  # sigma\n",
    "    r.tolist()[s],  # r\n",
    "    ([250] * len(K))[s],  # N\n",
    "    S0.tolist()[s],  # S0\n",
    "    right[s],  # option_type\n",
    "    ([True] * len(K))[s],  # american\n",
    "    ([0.0] * len(K))[s],  # dividend_yield\n",
    "    [dividends_res[i].schedule for i in range(len(dividends_res))][s],  # dividends,\n",
    "    dividend_type[s],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openbb_new_use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
